
@misc{2021,
  title = {Altmetric {{Attention Score}}},
  year = {2021},
  month = mar,
  journal = {Digital Science},
  abstract = {While the most important part of an Altmetric report is the~qualitative data, it's also useful to put attention in context and see how some research outputs are doing relative to others. The~Altmetric Attention Score~for a research output pro...},
  howpublished = {https://web.archive.org/web/20210418105109/https://help.altmetric.com/support/solutions/articles/6000233311-how-is-the-altmetric-attention-score-calculated-},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\RWW89232\\6000233311-how-is-the-altmetric-attention-score-calculated-.html}
}

@article{Ashar2021,
  title = {Brain Markers Predicting Response to Cognitive-behavioral Therapy for Social Anxiety Disorder: An Independent Replication of {{Whitfield}}-{{Gabrieli}} et al. 2015},
  shorttitle = {Brain Markers Predicting Response to Cognitive-behavioral Therapy for Social Anxiety Disorder},
  author = {Ashar, Yoni K. and Clark, Joseph and Gunning, Faith M. and Goldin, Philippe and Gross, James J. and Wager, Tor D.},
  year = {2021},
  month = jun,
  journal = {Translational Psychiatry},
  volume = {11},
  number = {1},
  pages = {260},
  issn = {2158-3188},
  doi = {10.1038/s41398-021-01366-y},
  abstract = {Abstract                            Predictive brain markers promise a number of important scientific, clinical, and societal applications. Over 600 predictive brain markers have been described in published reports, but very few have been tested in independent replication attempts. Here, we conducted an independent replication of a previously published marker predicting treatment response to cognitive-behavioral therapy for social anxiety disorder from patterns of resting-state fMRI amygdala connectivity               1               . The replication attempt was conducted in an existing dataset similar to the dataset used in the original report, by a team of independent investigators in consultation with the original authors. The precise model described in the original report positively predicted treatment outcomes in the replication dataset, but with marginal statistical significance, permutation test               p               \,=\,0.1. The effect size was substantially smaller in the replication dataset, with the model explaining 2\% of the variance in treatment outcomes, as compared to 21\% in the original report. Several lines of evidence, including the current replication attempt, suggest that features of amygdala function or structure may be able to predict treatment response in anxiety disorders. However, predictive models that explain a substantial amount of variance in independent datasets will be needed for scientific and clinical applications.},
  language = {en}
}

@article{Boekel2015,
  title = {A Purely Confirmatory Replication Study of Structural Brain-Behavior Correlations},
  author = {Boekel, Wouter and Wagenmakers, Eric-Jan and Belay, Luam and Verhagen, Josine and Brown, Scott and Forstmann, Birte U.},
  year = {2015},
  month = may,
  journal = {Cortex},
  volume = {66},
  pages = {115--133},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2014.11.019},
  abstract = {A recent `crisis of confidence' has emerged in the empirical sciences. Several studies have suggested that questionable research practices (QRPs) such as optional stopping and selective publication may be relatively widespread. These QRPs can result in a high proportion of false-positive findings, decreasing the reliability and replicability of research output. A potential solution is to register experiments prior to data acquisition and analysis. In this study we attempted to replicate studies that relate brain structure to behavior and cognition. These structural brain-behavior (SBB) correlations occasionally receive much attention in science and in the media. Given the impact of these studies, it is important to investigate their replicability. Here, we attempt to replicate five SBB correlation studies comprising a total of 17 effects. To prevent the impact of QRPs we employed a preregistered, purely confirmatory replication approach. For all but one of the 17 findings under scrutiny, confirmatory Bayesian hypothesis tests indicated evidence in favor of the null hypothesis ranging from anecdotal (Bayes factor~{$<~$}3) to strong (Bayes factor~{$>~$}10). In several studies, effect size estimates were substantially lower than in the original studies. To our knowledge, this is the first multi-study confirmatory replication of SBB correlations. With this study, we hope to encourage other researchers to undertake similar replication attempts.},
  language = {en},
  keywords = {Brain-behavior correlations,Confirmatory,Preregistration,Replication},
  file = {C\:\\Users\\peder\\Zotero\\storage\\Z46N59LU\\Boekel et al. - 2015 - A purely confirmatory replication study of structu.pdf;C\:\\Users\\peder\\Zotero\\storage\\G9GAS74M\\S0010945215000155.html}
}

@article{Bornmann2008,
  title = {What Do Citation Counts Measure? {{A}} Review of Studies on Citing Behavior},
  shorttitle = {What Do Citation Counts Measure?},
  author = {Bornmann, Lutz and Daniel, Hans-Dieter},
  year = {2008},
  month = jan,
  journal = {Journal of Documentation},
  volume = {64},
  number = {1},
  pages = {45--80},
  publisher = {{Emerald Group Publishing Limited}},
  issn = {0022-0418},
  doi = {10.1108/00220410810844150},
  abstract = {Purpose \textendash{} The purpose of this paper is to present a narrative review of studies on the citing behavior of scientists, covering mainly research published in the last 15 years. Based on the results of these studies, the paper seeks to answer the question of the extent to which scientists are motivated to cite a publication not only to acknowledge intellectual and cognitive influences of scientific peers, but also for other, possibly non-scientific, reasons. Design/methodology/approach \textendash{} The review covers research published from the early 1960s up to mid-2005 (approximately 30 studies on citing behavior-reporting results in about 40 publications). Findings \textendash{} The general tendency of the results of the empirical studies makes it clear that citing behavior is not motivated solely by the wish to acknowledge intellectual and cognitive influences of colleague scientists, since the individual studies reveal also other, in part non-scientific, factors that play a part in the decision to cite. However, the results of the studies must also be deemed scarcely reliable: the studies vary widely in design, and their results can hardly be replicated. Many of the studies have methodological weaknesses. Furthermore, there is evidence that the different motivations of citers are ``not so different or `randomly given' to such an extent that the phenomenon of citation would lose its role as a reliable measure of impact''. Originality/value \textendash{} Given the increasing importance of evaluative bibliometrics in the world of scholarship, the question ``What do citation counts measure?'' is a particularly relevant and topical issue.},
  keywords = {Bibliographic systems,Reference services},
  file = {C\:\\Users\\peder\\Zotero\\storage\\LP2A4NMS\\Bornmann and Daniel - 2008 - What do citation counts measure A review of studi.pdf;C\:\\Users\\peder\\Zotero\\storage\\2VU4BAZL\\html.html}
}

@article{Botvinik-Nezer2020,
  title = {Variability in the Analysis of a Single Neuroimaging Dataset by Many Teams},
  author = {{Botvinik-Nezer}, Rotem and Holzmeister, Felix and Camerer, Colin F. and Dreber, Anna and Huber, Juergen and Johannesson, Magnus and Kirchler, Michael and Iwanir, Roni and Mumford, Jeanette A. and Adcock, R. Alison and Avesani, Paolo and Baczkowski, Blazej M. and Bajracharya, Aahana and Bakst, Leah and Ball, Sheryl and Barilari, Marco and Bault, Nad{\`e}ge and Beaton, Derek and Beitner, Julia and Benoit, Roland G. and Berkers, Ruud M. W. J. and Bhanji, Jamil P. and Biswal, Bharat B. and {Bobadilla-Suarez}, Sebastian and Bortolini, Tiago and Bottenhorn, Katherine L. and Bowring, Alexander and Braem, Senne and Brooks, Hayley R. and Brudner, Emily G. and Calderon, Cristian B. and Camilleri, Julia A. and Castrellon, Jaime J. and Cecchetti, Luca and Cieslik, Edna C. and Cole, Zachary J. and Collignon, Olivier and Cox, Robert W. and Cunningham, William A. and Czoschke, Stefan and Dadi, Kamalaker and Davis, Charles P. and Luca, Alberto De and Delgado, Mauricio R. and Demetriou, Lysia and Dennison, Jeffrey B. and Di, Xin and Dickie, Erin W. and Dobryakova, Ekaterina and Donnat, Claire L. and Dukart, Juergen and Duncan, Niall W. and Durnez, Joke and Eed, Amr and Eickhoff, Simon B. and Erhart, Andrew and Fontanesi, Laura and Fricke, G. Matthew and Fu, Shiguang and Galv{\'a}n, Adriana and Gau, Remi and Genon, Sarah and Glatard, Tristan and Glerean, Enrico and Goeman, Jelle J. and Golowin, Sergej A. E. and {Gonz{\'a}lez-Garc{\'i}a}, Carlos and Gorgolewski, Krzysztof J. and Grady, Cheryl L. and Green, Mikella A. and Guassi Moreira, Jo{\~a}o F. and Guest, Olivia and Hakimi, Shabnam and Hamilton, J. Paul and Hancock, Roeland and Handjaras, Giacomo and Harry, Bronson B. and Hawco, Colin and Herholz, Peer and Herman, Gabrielle and Heunis, Stephan and Hoffstaedter, Felix and Hogeveen, Jeremy and Holmes, Susan and Hu, Chuan-Peng and Huettel, Scott A. and Hughes, Matthew E. and Iacovella, Vittorio and Iordan, Alexandru D. and Isager, Peder M. and Isik, Ayse I. and Jahn, Andrew and Johnson, Matthew R. and Johnstone, Tom and Joseph, Michael J. E. and Juliano, Anthony C. and Kable, Joseph W. and Kassinopoulos, Michalis and Koba, Cemal and Kong, Xiang-Zhen and Koscik, Timothy R. and Kucukboyaci, Nuri Erkut and Kuhl, Brice A. and Kupek, Sebastian and Laird, Angela R. and Lamm, Claus and Langner, Robert and Lauharatanahirun, Nina and Lee, Hongmi and Lee, Sangil and Leemans, Alexander and Leo, Andrea and Lesage, Elise and Li, Flora and Li, Monica Y. C. and Lim, Phui Cheng and Lintz, Evan N. and Liphardt, Schuyler W. and Losecaat Vermeer, Annabel B. and Love, Bradley C. and Mack, Michael L. and Malpica, Norberto and Marins, Theo and Maumet, Camille and McDonald, Kelsey and McGuire, Joseph T. and Melero, Helena and M{\'e}ndez Leal, Adriana S. and Meyer, Benjamin and Meyer, Kristin N. and Mihai, Glad and Mitsis, Georgios D. and Moll, Jorge and Nielson, Dylan M. and Nilsonne, Gustav and Notter, Michael P. and Olivetti, Emanuele and Onicas, Adrian I. and Papale, Paolo and Patil, Kaustubh R. and Peelle, Jonathan E. and P{\'e}rez, Alexandre and Pischedda, Doris and Poline, Jean-Baptiste and Prystauka, Yanina and Ray, Shruti and {Reuter-Lorenz}, Patricia A. and Reynolds, Richard C. and Ricciardi, Emiliano and Rieck, Jenny R. and {Rodriguez-Thompson}, Anais M. and Romyn, Anthony and Salo, Taylor and {Samanez-Larkin}, Gregory R. and {Sanz-Morales}, Emilio and Schlichting, Margaret L. and Schultz, Douglas H. and Shen, Qiang and Sheridan, Margaret A. and Silvers, Jennifer A. and Skagerlund, Kenny and Smith, Alec and Smith, David V. and {Sokol-Hessner}, Peter and Steinkamp, Simon R. and Tashjian, Sarah M. and Thirion, Bertrand and Thorp, John N. and Tingh{\"o}g, Gustav and Tisdall, Loreen and Tompson, Steven H. and {Toro-Serey}, Claudio and Torre Tresols, Juan Jesus and Tozzi, Leonardo and Truong, Vuong and Turella, Luca and {van `t Veer}, Anna E. and Verguts, Tom and Vettel, Jean M. and Vijayarajah, Sagana and Vo, Khoi and Wall, Matthew B. and Weeda, Wouter D. and Weis, Susanne and White, David J. and Wisniewski, David and {Xifra-Porxas}, Alba and Yearling, Emily A. and Yoon, Sangsuk and Yuan, Rui and Yuen, Kenneth S. L. and Zhang, Lei and Zhang, Xu and Zosky, Joshua E. and Nichols, Thomas E. and Poldrack, Russell A. and Schonberg, Tom},
  year = {2020},
  month = jun,
  journal = {Nature},
  volume = {582},
  number = {7810},
  pages = {84--88},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2314-9},
  abstract = {Data analysis workflows in many scientific domains have become increasingly complex and flexible. Here we assess the effect of this flexibility on the results of functional magnetic resonance imaging~by asking 70 independent teams to analyse the same dataset, testing the same 9 ex-ante hypotheses1. The flexibility of analytical approaches is exemplified by the fact that no two teams chose identical workflows to analyse the data. This flexibility resulted in sizeable variation in the results of hypothesis tests, even for teams whose statistical maps were highly correlated at intermediate stages of the analysis pipeline. Variation in reported results was related to several aspects of analysis methodology. Notably, a meta-analytical approach that aggregated information across teams yielded a significant consensus in activated regions. Furthermore, prediction markets of researchers in the field revealed an overestimation of the likelihood of significant findings, even by researchers with direct knowledge of the dataset2\textendash 5. Our findings show that analytical flexibility can have substantial effects on scientific conclusions, and identify factors that may be related to variability in the analysis of functional magnetic resonance imaging. The results emphasize the importance of validating and sharing complex analysis workflows, and demonstrate the need for performing and reporting multiple analyses of the same data. Potential approaches that could be used to mitigate issues related to analytical variability are discussed.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  language = {en},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Decision;Decision making;Human behaviour;Scientific community Subject\_term\_id: decision;decision-making;human-behaviour;scientific-community},
  file = {C\:\\Users\\peder\\Zotero\\storage\\4VQA6IX5\\Botvinik-Nezer et al. - 2020 - Variability in the analysis of a single neuroimagi.pdf;C\:\\Users\\peder\\Zotero\\storage\\XYGBD5K5\\s41586-020-2314-9.html}
}

@article{Brandt2014,
  title = {The {{Replication Recipe}}: {{What}} Makes for a Convincing Replication?},
  shorttitle = {The {{Replication Recipe}}},
  author = {Brandt, Mark J. and IJzerman, Hans and Dijksterhuis, Ap and Farach, Frank J. and Geller, Jason and {Giner-Sorolla}, Roger and Grange, James A. and Perugini, Marco and Spies, Jeffrey R. and {van 't Veer}, Anna},
  year = {2014},
  month = jan,
  journal = {Journal of Experimental Social Psychology},
  volume = {50},
  pages = {217--224},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2013.10.005},
  abstract = {Psychological scientists have recently started to reconsider the importance of close replications in building a cumulative knowledge base; however, there is no consensus about what constitutes a convincing close replication study. To facilitate convincing close replication attempts we have developed a Replication Recipe, outlining standard criteria for a convincing close replication. Our Replication Recipe can be used by researchers, teachers, and students to conduct meaningful replication studies and integrate replications into their scholarly habits.},
  keywords = {Pre-registration,Replication,Research method,Solid Science,Statistical power},
  file = {C\:\\Users\\peder\\Zotero\\storage\\LDH4M39D\\Brandt et al. - 2014 - The Replication Recipe What makes for a convincin.pdf;C\:\\Users\\peder\\Zotero\\storage\\Z4MMDUKN\\S0022103113001819.html}
}

@article{Burman2010,
  title = {A {{Call}} for {{Replication Studies}}},
  author = {Burman, Leonard E. and Reed, W. Robert and Alm, James},
  year = {2010},
  month = nov,
  journal = {Public Finance Review},
  volume = {38},
  number = {6},
  pages = {787--793},
  issn = {1091-1421, 1552-7530},
  doi = {10.1177/1091142110385210},
  language = {en}
}

@article{Button2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\CG6JAVBE\\Button et al. - 2013 - Power failure why small sample size undermines th.pdf;C\:\\Users\\peder\\Zotero\\storage\\DNQHTJ7N\\nrn3475.html}
}

@article{Carp2012,
  title = {On the {{Plurality}} of ({{Methodological}}) {{Worlds}}: {{Estimating}} the {{Analytic Flexibility}} of {{fMRI Experiments}}},
  shorttitle = {On the {{Plurality}} of ({{Methodological}}) {{Worlds}}},
  author = {Carp, Joshua},
  year = {2012},
  journal = {Frontiers in Neuroscience},
  volume = {6},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2012.00149},
  abstract = {How likely are published findings in the functional neuroimaging literature to be false? According to a recent mathematical model, the potential for false positives increases with the flexibility of analysis methods. Functional MRI (fMRI) experiments can be analyzed using a large number of commonly used tools, with little consensus on how, when, or whether to apply each one. This situation may lead to substantial variability in analysis outcomes. Thus, the present study sought to estimate the flexibility of neuroimaging analysis by submitting a single event-related fMRI experiment to a large number of unique analysis procedures. Ten analysis steps for which multiple strategies appear in the literature were identified, and two to four strategies were enumerated for each step. Considering all possible combinations of these strategies yielded 6,912 unique analysis pipelines. Activation maps from each pipeline were corrected for multiple comparisons using five thresholding approaches, yielding 34,560 significance maps. While some outcomes were relatively consistent across pipelines, others showed substantial methods-related variability in activation strength, location, and extent. Some analysis decisions contributed to this variability more than others, and different decisions were associated with distinct patterns of variability across the brain. Qualitative outcomes also varied with analysis parameters: many contrasts yielded significant activation under some pipelines but not others. Altogether, these results reveal considerable flexibility in the analysis of fMRI experiments. This observation, when combined with mathematical simulations linking analytic flexibility with elevated false positive rates, suggests that false positive results may be more prevalent than expected in the literature. This risk of inflated false positive rates may be mitigated by constraining the flexibility of analytic choices or by abstaining from selective analysis reporting.},
  language = {English},
  file = {C\:\\Users\\peder\\Zotero\\storage\\HBZ9W5AC\\Carp - 2012 - On the Plurality of (Methodological) Worlds Estim.pdf;C\:\\Users\\peder\\Zotero\\storage\\WMFXJDTT\\full.html}
}

@misc{Chamberlain2020,
  title = {Rcrossref},
  author = {Chamberlain, Scott and Zhu, Hao and Jahn, Najko and Boettiger, Carl and Ram, Karthik},
  year = {2020},
  month = oct,
  abstract = {Client for various 'CrossRef' 'APIs', including 'metadata' search with their old and newer search 'APIs', get 'citations' in various formats (including 'bibtex', 'citeproc-json', 'rdf-xml', etc.), convert 'DOIs' to 'PMIDs', and 'vice versa', get citations for 'DOIs', and get links to full text of articles when available.},
  file = {C\:\\Users\\peder\\Zotero\\storage\\KPATE2N3\\index.html}
}

@article{Chi2021,
  title = {Examining the Correlation between {{Altmetric Attention Score}} and Citation Count in the Gynecologic Oncology Literature: {{Does}} It Have an Impact?},
  shorttitle = {Examining the Correlation between {{Altmetric Attention Score}} and Citation Count in the Gynecologic Oncology Literature},
  author = {Chi, Andrew J. and Lopes, Alexandra J. and Rong, Lisa Q. and Charlson, Mary E. and Alvarez, Ronald D. and Boerner, Thomas},
  year = {2021},
  month = aug,
  journal = {Gynecologic Oncology Reports},
  volume = {37},
  pages = {100778},
  issn = {2352-5789},
  doi = {10.1016/j.gore.2021.100778},
  abstract = {We sought to determine the correlation between Altmetric Attention Score and traditional bibliometrics in the gynecologic oncology literature. We identified the 10 most-cited gynecologic oncology articles from 5 major gynecology journals and 10 major ``oncology'' journals that publish on gynecologic oncology during 2014, 2016, and 2018. Article citation count and Altmetric Attention Score (AAS), as well as journal impact factor (IF) and date of Twitter account development were recorded. Pearson's correlation coefficient was used to describe the relationship between AAS, tweets, IF, and citation count. While the median citation counts significantly decreased for the top-cited gynecologic oncology articles from 2014 to 2018 (p~{$<~$}0.001), the corresponding median AAS continuously increased during this period (p~=~0.008). For articles published in 2014 and 2018, there was a strong positive relationship between the median citation count and the median AAS (2014: r~=~0.92; 2018: r~=~0.97), as well as between the IF (r~=~0.78 and r~=~0.89, respectively); these correlations were moderate to weak in 2016 (r~=~0.5 and r~=~0.41, respectively). There was a continuously increasing strong positive correlation from 2014 to 2018 between journal IF and median AAS (2014: r~=~0.75; 2016: r~=~0.82; 2018: r~=~0.92). Gynecologic oncology articles published in higher impact journals are associated with increased social media visibility and attention. Our data support the idea that early online attention scores, like the AAS, might be useful for predicting future citation counts for oncology publications in general and gynecologic oncology specifically.},
  language = {en},
  keywords = {Altmetric Attention Score,Altmetrics,Citation count,Gynecologic oncology literature,Impact factor,Twitter},
  file = {C\:\\Users\\peder\\Zotero\\storage\\J65ABFPN\\Chi et al. - 2021 - Examining the correlation between Altmetric Attent.pdf;C\:\\Users\\peder\\Zotero\\storage\\UUDATZER\\S2352578921000837.html}
}

@article{Colman2015,
  title = {Memory and Burstiness in Dynamic Networks},
  author = {Colman, Ewan R. and Vukadinovi{\'c} Greetham, Danica},
  year = {2015},
  month = jul,
  journal = {Physical Review E},
  volume = {92},
  number = {1},
  pages = {012817},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.92.012817},
  abstract = {A discrete-time random process is described, which can generate bursty sequences of events. A Bernoulli process, where the probability of an event occurring at time t is given by a fixed probability x, is modified to include a memory effect where the event probability is increased proportionally to the number of events that occurred within a given amount of time preceding t. For small values of x the interevent time distribution follows a power law with exponent -2-x. We consider a dynamic network where each node forms, and breaks connections according to this process. The value of x for each node depends on the fitness distribution, {$\rho$}(x), from which it is drawn; we find exact solutions for the expectation of the degree distribution for a variety of possible fitness distributions, and for both cases where the memory effect either is, or is not present. This work can potentially lead to methods to uncover hidden fitness distributions from fast changing, temporal network data, such as online social communications and fMRI scans.},
  file = {C\:\\Users\\peder\\Zotero\\storage\\HRW4YQ3I\\Colman and Vukadinović Greetham - 2015 - Memory and burstiness in dynamic networks.pdf;C\:\\Users\\peder\\Zotero\\storage\\F7N8KK6I\\PhysRevE.92.html}
}

@article{Costas2015,
  title = {Do ``Altmetrics'' Correlate with Citations? {{Extensive}} Comparison of Altmetric Indicators with Citations from a Multidisciplinary Perspective},
  shorttitle = {Do ``Altmetrics'' Correlate with Citations?},
  author = {Costas, Rodrigo and Zahedi, Zohreh and Wouters, Paul},
  year = {2015},
  journal = {Journal of the Association for Information Science and Technology},
  volume = {66},
  number = {10},
  pages = {2003--2019},
  issn = {2330-1643},
  doi = {10.1002/asi.23309},
  abstract = {An extensive analysis of the presence of different altmetric indicators provided by Altmetric.com across scientific fields is presented, particularly focusing on their relationship with citations. Our results confirm that the presence and density of social media altmetric counts are still very low and not very frequent among scientific publications, with 15\%\textendash 24\% of the publications presenting some altmetric activity and concentrated on the most recent publications, although their presence is increasing over time. Publications from the social sciences, humanities, and the medical and life sciences show the highest presence of altmetrics, indicating their potential value and interest for these fields. The analysis of the relationships between altmetrics and citations confirms previous claims of positive correlations but is relatively weak, thus supporting the idea that altmetrics do not reflect the same kind of impact as citations. Also, altmetric counts do not always present a better filtering of highly-cited publications than journal citation scores. Altmetric scores (particularly mentions in blogs) are able to identify highly-cited publications with higher levels of precision than journal citation scores (JCS), but they have a lower level of recall. The value of altmetrics as a complementary tool of citation analysis is highlighted, although more research is suggested to disentangle the potential meaning and value of altmetric indicators for research evaluation.},
  copyright = {\textcopyright{} 2014 ASIS\&T},
  language = {en},
  keywords = {bibliometrics,databases},
  file = {C\:\\Users\\peder\\Zotero\\storage\\H5C9UTMZ\\Costas et al. - 2015 - Do “altmetrics” correlate with citations Extensiv.pdf;C\:\\Users\\peder\\Zotero\\storage\\DM35YBXU\\asi.html}
}

@article{DeVries2018,
  title = {Friends with Benefits: {{Behavioral}} and {{fMRI}} Studies on the Effect of Friendship Reminders on Self-Control for Compulsive and Non-Compulsive Buyers},
  shorttitle = {Friends with Benefits},
  author = {De Vries, Eline L. E. and Fennis, Bob M. and Bijmolt, Tammo H. A. and Ter Horst, Gert J. and Marsman, Jan-Bernard C.},
  year = {2018},
  month = jun,
  journal = {International Journal of Research in Marketing},
  volume = {35},
  number = {2},
  pages = {336--358},
  issn = {0167-8116},
  doi = {10.1016/j.ijresmar.2017.12.004},
  abstract = {Does the real or imagined presence of friends invariantly drive consumers to engage in disinhibited behavior, and give in to the ``urge to splurge'' in the face of consumption temptations? Or might there be situations in which being with friends or even merely thinking of friends or the friendships we have with them can actually improve self-control? In five studies, using a unique combination of controlled experiments examining overt consumer behavior and functional magnetic resonance imaging (fMRI), we propose and show that the extent to which consumers identify a goal conflict between giving in to buying temptations on the one hand and the perceived consequences for maintaining satisfactory relationships with close friends on the other is a critical mediator of whether friendship reminders decrease or increase self-control. We further show that such a goal conflict is most likely for consumers with a chronic, compulsive tendency for uncontrolled, disinhibited acquisition and consumption\textemdash for consumers classified as compulsive buyers. For their non-compulsive counterparts, in contrast, acts of acquisition and consumption, even incidental disinhibited ones, are perceived to be less problematic in light of their friendships and hence do not induce a goal conflict to the same extent. Our findings provide insights into social influences on self-control and identify the concept of friendship reminders as a way to reduce a common type of dysfunctional consumer behavior. In addition to enhancing consumer well-being, reducing compulsive buying will substantially reduce handling costs for organizations. Hence, the findings are of academic, societal and managerial relevance.},
  language = {en},
  keywords = {Compulsive buying,fMRI,Friendship,Self-control,Social influence},
  file = {C\:\\Users\\peder\\Zotero\\storage\\49UATFU8\\S0167811617300861.html}
}

@article{Dimoka2011,
  title = {{{NeuroIS}}: {{The Potential}} of {{Cognitive Neuroscience}} for {{Information Systems Research}}},
  shorttitle = {{\textbf{Research }}{{{\textbf{Commentary}}}} \textemdash{{NeuroIS}}},
  author = {Dimoka, Angelika and Pavlou, Paul A. and Davis, Fred D.},
  year = {2011},
  month = dec,
  journal = {Information Systems Research},
  volume = {22},
  number = {4},
  pages = {687--702},
  issn = {1047-7047, 1526-5536},
  doi = {10.1287/isre.1100.0284},
  language = {en}
}

@article{Federer2018,
  title = {Data Sharing in {{PLOS ONE}}: {{An}} Analysis of {{Data Availability Statements}}},
  shorttitle = {Data Sharing in {{PLOS ONE}}},
  author = {Federer, Lisa M. and Belter, Christopher W. and Joubert, Douglas J. and Livinski, Alicia and Lu, Ya-Ling and Snyders, Lissa N. and Thompson, Holly},
  year = {2018},
  month = may,
  journal = {PLOS ONE},
  volume = {13},
  number = {5},
  pages = {e0194768},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0194768},
  abstract = {A number of publishers and funders, including PLOS, have recently adopted policies requiring researchers to share the data underlying their results and publications. Such policies help increase the reproducibility of the published literature, as well as make a larger body of data available for reuse and re-analysis. In this study, we evaluate the extent to which authors have complied with this policy by analyzing Data Availability Statements from 47,593 papers published in PLOS ONE between March 2014 (when the policy went into effect) and May 2016. Our analysis shows that compliance with the policy has increased, with a significant decline over time in papers that did not include a Data Availability Statement. However, only about 20\% of statements indicate that data are deposited in a repository, which the PLOS policy states is the preferred method. More commonly, authors state that their data are in the paper itself or in the supplemental information, though it is unclear whether these data meet the level of sharing required in the PLOS policy. These findings suggest that additional review of Data Availability Statements or more stringent policies may be needed to increase data sharing.},
  language = {en},
  keywords = {Biotechnology,Institutional repositories,Open data,Public policy,Randomized controlled trials,Reproducibility,Science policy,Scientific publishing},
  file = {C\:\\Users\\peder\\Zotero\\storage\\RFMQYA2G\\Federer et al. - 2018 - Data sharing in PLOS ONE An analysis of Data Avai.pdf;C\:\\Users\\peder\\Zotero\\storage\\4PB8JFJX\\article.html}
}

@article{Field2019,
  title = {When and {{Why}} to {{Replicate}}: {{As Easy}} as 1, 2, 3?},
  shorttitle = {When and {{Why}} to {{Replicate}}},
  author = {Field, Sarahanne M. and Hoekstra, Rink and Bringmann, Laura and Van Ravenzwaaij, Don},
  year = {2019},
  month = sep,
  journal = {Collabra: Psychology},
  volume = {5},
  number = {1},
  pages = {46},
  issn = {2474-7394},
  doi = {10.1525/collabra.218},
  file = {C\:\\Users\\peder\\Zotero\\storage\\576NQHIY\\Field et al. - 2019 - When and Why to Replicate As Easy as 1, 2, 3.pdf}
}

@article{Forsell2019,
  title = {Predicting Replication Outcomes in the {{Many Labs}} 2 Study},
  author = {Forsell, Eskil and Viganola, Domenico and Pfeiffer, Thomas and Almenberg, Johan and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus and Dreber, Anna},
  year = {2019},
  month = dec,
  journal = {Journal of Economic Psychology},
  series = {Replications in {{Economic Psychology}} and {{Behavioral Economics}}},
  volume = {75},
  pages = {102117},
  issn = {0167-4870},
  doi = {10.1016/j.joep.2018.10.009},
  abstract = {Understanding and improving reproducibility is crucial for scientific progress. Prediction markets and related methods of eliciting peer beliefs are promising tools to predict replication outcomes. We invited researchers in the field of psychology to judge the replicability of 24 studies replicated in the large scale Many Labs 2 project. We elicited peer beliefs in prediction markets and surveys about two replication success metrics: the probability that the replication yields a statistically significant effect in the original direction (p {$<$} 0.001), and the relative effect size of the replication. The prediction markets correctly predicted 75\% of the replication outcomes, and were highly correlated with the replication outcomes. Survey beliefs were also significantly correlated with replication outcomes, but had larger prediction errors. The prediction markets for relative effect sizes attracted little trading and thus did not work well. The survey beliefs about relative effect sizes performed better and were significantly correlated with observed relative effect sizes. The results suggest that replication outcomes can be predicted and that the elicitation of peer beliefs can increase our knowledge about scientific reproducibility and the dynamics of hypothesis testing.},
  language = {en},
  keywords = {Beliefs,Prediction markets,Replications,Reproducibility},
  file = {C\:\\Users\\peder\\Zotero\\storage\\LJ4N73RY\\Forsell et al. - 2019 - Predicting replication outcomes in the Many Labs 2.pdf;C\:\\Users\\peder\\Zotero\\storage\\UG5IC8AA\\S0167487018303283.html}
}

@article{Furukawa2006,
  title = {Imputing Missing Standard Deviations in Meta-Analyses Can Provide Accurate Results},
  author = {Furukawa, Toshi A. and Barbui, Corrado and Cipriani, Andrea and Brambilla, Paolo and Watanabe, Norio},
  year = {2006},
  month = jan,
  journal = {Journal of Clinical Epidemiology},
  volume = {59},
  number = {1},
  pages = {7--10},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2005.06.006},
  abstract = {Background and Objectives Many reports of randomized controlled trials (RCTs) fail to provide standard deviations (SDs) of their continuous outcome measures. Some meta-analysts substitute them by those reported in other studies, either from another meta-analysis or from other studies in the same meta-analysis. But the validity of such practices has never been empirically examined. Methods We compared the actual standardized mean difference (SMD) of individual RCTs and the meta-analytically pooled SMD of all RCTs against those based on the above-mentioned two imputation methods in two meta-analyses of antidepressants. Results Two meta-analyses included 39 RCTs of fluoxetine (n = 3,681) and 25 RCTs of amitriptyline (n = 1,832), which had actually reported means and SDs of the Hamilton Rating Scale for Depression. According to either of the two proposed imputation methods, the agreement between actual SMDs and imputed SMDs for individual RCTs was very good with ANOVA intraclass correlation coefficients between 0.61 and 0.97. The agreement between the actual pooled SMD and the imputed one was even better, with minimal differences in both their point estimates and 95\% confidence intervals. Conclusion For a systematic review where some of the identified trials do not report SDs, it appears safe to borrow SDs from other studies.},
  language = {en},
  keywords = {Depressive disorder,Imputation,Meta-analysis,Missing data,Standard deviation},
  file = {C\:\\Users\\peder\\Zotero\\storage\\KP2AHRE6\\Furukawa et al. - 2006 - Imputing missing standard deviations in meta-analy.pdf;C\:\\Users\\peder\\Zotero\\storage\\WV32FFI5\\S0895435605003227.html}
}

@article{Glasziou2008,
  title = {What Is Missing from Descriptions of Treatment in Trials and Reviews?},
  author = {Glasziou, Paul and Meats, Emma and Heneghan, Carl and Shepperd, Sasha},
  year = {2008},
  month = jun,
  journal = {BMJ},
  volume = {336},
  number = {7659},
  pages = {1472--1474},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {0959-8138, 1756-1833},
  doi = {10.1136/bmj.39590.732037.47},
  abstract = {{$<$}p{$>$}Replicating non-pharmacological treatments in practice depends on how well they have been described in research studies{$<$}/p{$>$}},
  chapter = {Analysis},
  copyright = {\textcopyright{} BMJ Publishing Group Ltd 2008},
  language = {en},
  pmid = {18583680},
  file = {C\:\\Users\\peder\\Zotero\\storage\\ZIMFUL6S\\Glasziou et al. - 2008 - What is missing from descriptions of treatment in .pdf;C\:\\Users\\peder\\Zotero\\storage\\TG4E3MKJ\\1472.html}
}

@article{Harms2018,
  title = {Making 'null Effects' Informative: Statistical Techniques and Inferential Frameworks},
  shorttitle = {Making 'null Effects' Informative},
  author = {Harms, Christopher and Lakens, Dani{\"e}l},
  year = {2018},
  month = jul,
  journal = {Journal of Clinical and Translational Research},
  volume = {3},
  number = {Suppl 2},
  pages = {382--393},
  issn = {2382-6533},
  abstract = {Being able to interpret `null effects?is important for cumulative knowledge generation in science. To draw informative conclusions from null-effects, researchers need to move beyond the incorrect interpretation of a non-significant result in a null-hypothesis significance test as evidence of the absence of an effect. We explain how to statistically evaluate null-results using equivalence tests, Bayesian estimation, and Bayes factors. A worked example demonstrates how to apply these statistical tools and interpret the results. Finally, we explain how no statistical approach can actually prove that the null-hypothesis is true, and briefly discuss the philosophical differences between statistical approaches to examine null-effects. The increasing availability of easy-to-use software and online tools to perform equivalence tests, Bayesian estimation, and calculate Bayes factors make it timely and feasible to complement or move beyond traditional null-hypothesis tests, and allow researchers to draw more informative conclusions about null-effects.},
  pmcid = {PMC6412612},
  pmid = {30873486},
  file = {C\:\\Users\\peder\\Zotero\\storage\\DMMVX546\\Harms and Lakens - 2018 - Making 'null effects' informative statistical tec.pdf}
}

@article{Heirene2021,
  title = {A Call for Replications of Addiction Research: Which Studies Should We Replicate and What Constitutes a `Successful' Replication?},
  shorttitle = {A Call for Replications of Addiction Research},
  author = {Heirene, Robert M.},
  year = {2021},
  month = mar,
  journal = {Addiction Research \& Theory},
  volume = {29},
  number = {2},
  pages = {89--97},
  publisher = {{Taylor \& Francis}},
  issn = {1606-6359},
  doi = {10.1080/16066359.2020.1751130},
  abstract = {Several prominent researchers in the problem gambling field have recently called for high-quality replications of existing gambling studies. This call should be extended to the entire field of addiction research: there is a need to focus on ensuring that the understanding of addiction and related phenomena gained through the extant literature is robust and replicable. This article discusses two important questions addictions researchers should consider before proceeding with replication studies: [1] which studies should we attempt to replicate? And: [2] how should we interpret the findings of a replication study in relation to the original study? In answering these questions, a focus is placed on experimental research, though the discussion may still serve as a useful introduction to the topic of replications for addictions researchers using any methodology.},
  keywords = {addiction,methods,open science,Replication,reproducibility},
  annotation = {\_eprint: https://doi.org/10.1080/16066359.2020.1751130},
  file = {C\:\\Users\\peder\\Zotero\\storage\\38V57DQD\\Heirene - 2021 - A call for replications of addiction research whi.pdf;C\:\\Users\\peder\\Zotero\\storage\\QEARLYXI\\16066359.2020.html}
}

@book{Hernan2020,
  title = {Causal {{Inference}}: {{What If}}.},
  author = {Hern{\'a}n, Miguel and Robins, James},
  year = {2020},
  publisher = {{Chapman \& Hall/CRC}},
  address = {{Boca Raton}}
}

@misc{Herve2021,
  title = {{{RVAideMemoire}}: {{Testing}} and {{Plotting Procedures}} for {{Biostatistics}}},
  shorttitle = {{{RVAideMemoire}}},
  author = {Herv{\'e}, Maxime},
  year = {2021},
  month = jun,
  abstract = {Contains miscellaneous functions useful in biostatistics, mostly univariate and multivariate testing procedures with a special emphasis on permutation tests. Many functions intend to simplify user's life by shortening existing procedures or by implementing plotting functions that can be used with as many methods from different packages as possible.},
  copyright = {GPL-2}
}

@article{Horster2020,
  title = {A {{Neglected Topic}} in {{Neuroscience}}: {{Replicability}} of {{fMRI Results With Specific Reference}} to {{ANOREXIA NERVOSA}}},
  shorttitle = {A {{Neglected Topic}} in {{Neuroscience}}},
  author = {Horster, Isabelle and Nickel, Kathrin and Holovics, Lukas and Schmidt, Stefan and Endres, Dominique and {Tebartz van Elst}, Ludger and Zeeck, Almut and Maier, Simon and Joos, Andreas},
  year = {2020},
  month = aug,
  journal = {Frontiers in Psychiatry},
  volume = {11},
  issn = {1664-0640},
  doi = {10.3389/fpsyt.2020.00777},
  abstract = {Functional magnetic resonance imaging (fMRI) studies report impaired functional correlates of cognition and emotion in mental disorders. The validity of preexisting studies needs to be confirmed through replication studies, which there is a lack of. So far, most replication studies have been conducted on non-patients (NP) and primarily investigated cognitive and motor tasks. To fill this gap, we conducted the first fMRI replication study to investigate brain function using disease-related food stimuli in patients with anorexia nervosa (AN). Using fMRI, we investigated 31 AN patients and 27 NP for increased amygdala and reduced midcingulate activation when viewing food and non-food stimuli, as reported by the original study (11AN, 11NP; Joos et~al., 2011). Similar to the previous study, we observed in the within group comparisons (food{$>$}non-food) a frontoinsular activation for both groups. Although in AN the recorded activation clustered more prominently and extended into the cingulate cortex. In the between-group comparisons, the increased amygdala and reduced midcingulate activation could not be replicated. Instead, AN showed a higher activation of the cingulate cortices, the pre-/postcentral gyrus and the inferior parietal lobe. Unlike in the initial study, no significant differences between NP{$>$}AN could be observed. The inconsistency of results and the non-replication of the study could have several reasons, such as high inter-individual variance of functional correlates of emotion processing, as well as intra-individual variances and the smaller group size of the initial study. These results underline the importance of replication for assessing the reliability and validity of results from fMRI research.},
  pmcid = {PMC7419696},
  pmid = {32848943},
  file = {C\:\\Users\\peder\\Zotero\\storage\\73ZEDJPU\\Horster et al. - 2020 - A Neglected Topic in Neuroscience Replicability o.pdf}
}

@article{Huber2019,
  title = {Less ``{{Story}}'' and More ``{{Reliability}}'' in Cognitive Neuroscience},
  author = {Huber, David E. and Potter, Kevin W. and Huszar, Lucas D.},
  year = {2019},
  month = apr,
  journal = {Cortex; a journal devoted to the study of the nervous system and behavior},
  volume = {113},
  pages = {347--349},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2018.10.030},
  pmcid = {PMC6702667},
  pmid = {30638584},
  file = {C\:\\Users\\peder\\Zotero\\storage\\XL6N5UTK\\Huber et al. - 2019 - Less “Story” and more “Reliability” in cognitive n.pdf}
}

@article{Isager2018,
  title = {What {{To Replicate}}? {{Justifications Of Study Choice From}} 85 {{Replication Studies}}.},
  shorttitle = {What {{To Replicate}}?},
  author = {Isager, Peder M.},
  year = {2018},
  month = jun,
  doi = {10.5281/zenodo.1286715},
  abstract = {This is the registered version of the blog post "what to replicate?" published 10-06-2018 at https://pedermisager.netlify.com/post/what-to-replicate/ The registration contains a PDF version of the blog post content, as well as a copy of the Google spreadsheet linked to in blog post. The spreadsheet was copied 11-06-2018.},
  language = {en}
}

@article{Isager2021,
  title = {Deciding What to Replicate: {{A}} Decision Model for Replication Study Selection under Resource and Knowledge Constraints},
  shorttitle = {Deciding What to Replicate},
  author = {Isager, Peder Mortvedt and van Aert, Robbie C. M. and Bahn\'ik, \v{S}t\v{e}p\'an and Brandt, Mark J. and DeSoto, K. Andrew and Giner-Sorolla, Roger and Krueger, Joachim I. and Perugini, Marco and Ropovik, Ivan and van 't Veer, Anna E. and Vranka, Marek and Lakens, Dani\"el},
  options = {useprefix=true},
  date = {2021},
  journaltitle = {Psychological Methods},
  pages = {No Pagination Specified-No Pagination Specified},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1463},
  doi = {10.1037/met0000438},
  abstract = {Robust scientific knowledge is contingent upon replication of original findings. However, replicating researchers are constrained by resources, and will almost always have to choose one replication effort to focus on from a set of potential candidates. To select a candidate efficiently in these cases, we need methods for deciding which out of all candidates considered would be the most useful to replicate, given some overall goal researchers wish to achieve. In this article we assume that the overall goal researchers wish to achieve is to maximize the utility gained by conducting the replication study. We then propose a general rule for study selection in replication research based on the replication value of the set of claims considered for replication. The replication value of a claim is defined as the maximum expected utility we could gain by conducting a replication of the claim, and is a function of (a) the value of being certain about the claim, and (b) uncertainty about the claim based on current evidence. We formalize this definition in terms of a causal decision model, utilizing concepts from decision theory and causal graph modeling. We discuss the validity of using replication value as a measure of expected utility gain, and we suggest approaches for deriving quantitative estimates of replication value. Our goal in this article is not to define concrete guidelines for study selection, but to provide the necessary theoretical foundations on which such concrete guidelines could be built. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Concepts,Decision Theory,Experimental Replication,Models,Simulation,Test Validity,Uncertainty},
  file = {C\:\\Users\\dlakens\\Zotero\\storage\\Y7TBVABC\\Isager et al. - 2021 - Deciding what to replicate A decision model for r.pdf}
}


@misc{Isager2020a,
  title = {Test Validity Defined as D-Connection between Target and Measured Attribute: {{Expanding}} the Causal Definition of {{Borsboom}} et al. (2004)},
  shorttitle = {Test Validity Defined as D-Connection between Target and Measured Attribute},
  author = {Isager, Peder M.},
  year = {2020},
  month = sep,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/btgsr},
  abstract = {This article suggests a modification to the conception of test validity put forward by Borsboom, Mellenbergh and van Heerden (2004). According to the original definition, a test is only valid if test outcomes are caused by variation in the target attribute. According to the d-connection definition of test validity, a test is valid for measuring an attribute if (a) the attribute exists, and (b) variation in the attribute is d-connected to variation in the measurement outcomes. In other words, a test is valid whenever test outcomes inform us either about what has happened to the target attribute in the past, or about what will happen to the target attribute in the future. Thus, the d-connection definition expands the number of scenarios in which a test can be considered valid. Defining test validity as d-connection between target and measured attribute situates the validity concept squarely within the structural causal modeling framework of Pearl (2009).},
  keywords = {causal inference,d-connection,measurement,Psychometrics,Quantitative Methods,Social and Behavioral Sciences,test validity,Theory and Philosophy of Science,validity},
  file = {C\:\\Users\\peder\\Zotero\\storage\\4B8XQSS8\\Isager - 2020 - Test validity defined as d-connection between targ.pdf}
}

@misc{Isager2021,
  title = {Replication Value as a Function of Citation Impact and Sample Size},
  author = {Isager, Peder M. and van 't Veer, Anna E. and Lakens, Daniel},
  year = {2021},
  month = aug,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/knjea},
  abstract = {Researchers seeking to replicate original research often need to decide which of several relevant candidates to select for replication. Several strategies for study selection have been proposed, utilizing a variety of observed indicators as criteria for selection. However, few strategies clearly specify the goal of study selection and how that goal is related to the indicators that are utilized. We have previously formalized a decision model of replication study selection in which the goal of study selection is to maximize the expected utility gain of the replication e?ort. We further define the concept of replication value as a proxy for expected utility gain (Isager et al., 2020). In this article, we propose a quantitative operationalization of replication value. We first discuss how value and uncertainty - the two concepts used to determine replication value \textendash{} could be estimated via information about citation count and sample size. Second, we propose an equation for combining these indicators into an overall estimate of replication value, which we denote RVCn. Third, we suggest how RVCn could be implemented as part of a broader study selection procedure. Finally, we provide preliminary data suggesting that studies that were in fact selected for replication tend to have relatively high RVCn estimates. The goal of this article is to explain how RVCn is intended to work and, in doing so, demonstrate the many assumptions that should be explicit in any replication study selection strategy.},
  keywords = {citation count,Design of Experiments and Sample Surveys,Economics,expected utility,Other Economics,Physical Sciences and Mathematics,replication,replication value,RVcn,sample size,Social and Behavioral Sciences,Statistics and Probability,study selection},
  file = {C\:\\Users\\peder\\Zotero\\storage\\25DDS6ZW\\Isager et al. - 2021 - Replication value as a function of citation impact.pdf}
}

@article{Javadi2017,
  title = {Hippocampal and Prefrontal Processing of Network Topology to Simulate the Future},
  author = {Javadi, Amir-Homayoun and Emo, Beatrix and Howard, Lorelei R. and Zisch, Fiona E. and Yu, Yichao and Knight, Rebecca and Pinelo Silva, Joao and Spiers, Hugo J.},
  year = {2017},
  month = mar,
  journal = {Nature Communications},
  volume = {8},
  number = {1},
  pages = {14652},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/ncomms14652},
  abstract = {Topological networks lie at the heart of our cities and social milieu. However, it remains unclear how and when the brain processes topological structures to guide future behaviour during everyday life. Using fMRI in humans and a simulation of London (UK), here we show that, specifically when new streets are entered during navigation of the city, right posterior hippocampal activity indexes the change in the number of local topological connections available for future travel and right anterior hippocampal activity reflects global properties of the street entered. When forced detours require re-planning of the route to the goal, bilateral inferior lateral prefrontal activity scales with the planning demands of a breadth-first search of future paths. These results help shape models of how hippocampal and prefrontal regions support navigation, planning and future simulation.},
  copyright = {2017 The Author(s)},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\8YSCYH2E\\Javadi et al. - 2017 - Hippocampal and prefrontal processing of network t.pdf;C\:\\Users\\peder\\Zotero\\storage\\W48RRM57\\ncomms14652.html}
}

@article{Kanwisher1997,
  title = {The {{Fusiform Face Area}}: {{A Module}} in {{Human Extrastriate Cortex Specialized}} for {{Face Perception}}},
  shorttitle = {The {{Fusiform Face Area}}},
  author = {Kanwisher, Nancy and McDermott, Josh and Chun, Marvin M.},
  year = {1997},
  month = jun,
  journal = {Journal of Neuroscience},
  volume = {17},
  number = {11},
  pages = {4302--4311},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.17-11-04302.1997},
  abstract = {Using functional magnetic resonance imaging (fMRI), we found an area in the fusiform gyrus in 12 of the 15 subjects tested that was significantly more active when the subjects viewed faces than when they viewed assorted common objects. This face activation was used to define a specific region of interest individually for each subject, within which several new tests of face specificity were run. In each of five subjects tested, the predefined candidate ``face area'' also responded significantly more strongly to passive viewing of (1) intact than scrambled two-tone faces, (2) full front-view face photos than front-view photos of houses, and (in a different set of five subjects) (3) three-quarter-view face photos (with hair concealed) than photos of human hands; it also responded more strongly during (4) a consecutive matching task performed on three-quarter-view faces versus hands. Our technique of running multiple tests applied to the same region defined functionally within individual subjects provides a solution to two common problems in functional imaging: (1) the requirement to correct for multiple statistical comparisons and (2) the inevitable ambiguity in the interpretation of any study in which only two or three conditions are compared. Our data allow us to reject alternative accounts of the function of the fusiform face area (area ``FF'') that appeal to visual attention, subordinate-level classification, or general processing of any animate or human forms, demonstrating that this region is selectively involved in the perception of faces.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 1997 Society for Neuroscience},
  language = {en},
  pmid = {9151747},
  keywords = {extrastriate cortex,face perception,functional MRI,fusiform gyrus,object recognition,ventral visual pathway},
  file = {C\:\\Users\\peder\\Zotero\\storage\\T4DRDYEZ\\Kanwisher et al. - 1997 - The Fusiform Face Area A Module in Human Extrastr.pdf;C\:\\Users\\peder\\Zotero\\storage\\3YKRSYEF\\4302.html}
}

@article{Kassam2013,
  title = {Identifying {{Emotions}} on the {{Basis}} of {{Neural Activation}}},
  author = {Kassam, Karim S. and Markey, Amanda R. and Cherkassky, Vladimir L. and Loewenstein, George and Just, Marcel Adam},
  editor = {Gray, Marcus},
  year = {2013},
  month = jun,
  journal = {PLoS ONE},
  volume = {8},
  number = {6},
  pages = {e66032},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0066032},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\4W2YJQBW\\Kassam et al. - 2013 - Identifying Emotions on the Basis of Neural Activa.pdf}
}

@article{Kipman2012,
  title = {A Funny Thing Happened on the Way to the Scanner: Humor Detection Correlates with Gray Matter Volume},
  shorttitle = {A Funny Thing Happened on the Way to the Scanner},
  author = {Kipman, Maia and Weber, Mareen and Schwab, Zachary J. and DelDonno, Sophie R. and Killgore, William D. S.},
  year = {2012},
  month = dec,
  journal = {NeuroReport},
  volume = {23},
  number = {18},
  pages = {1059--1064},
  issn = {0959-4965},
  doi = {10.1097/WNR.0b013e32835ad307},
  language = {en}
}

@article{Klein2018,
  title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability Across Samples}} and {{Settings}}},
  shorttitle = {Many {{Labs}} 2},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and {de Bruijn}, Maaike and De Schutter, Leander and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\AA}se H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\dj}edovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Lee Nichols, Austin and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and {V{\'a}squez- Echeverr{\'i}a}, Alejandro and Ann Vaughn, Leigh and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {4},
  pages = {443--490},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245918810225},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\PGUL7MDU\\Klein et al. - 2018 - Many Labs 2 Investigating Variation in Replicabil.pdf}
}

@book{KNAW2018,
  title = {Replication Studies \textendash{} {{Improving}} Reproducibility in the Empirical Sciences},
  author = {KNAW},
  year = {2018},
  publisher = {{KNAW}},
  address = {{Amsterdam}},
  isbn = {978-90-6984-720-7}
}

@article{Koo2016,
  title = {A {{Guideline}} of {{Selecting}} and {{Reporting Intraclass Correlation Coefficients}} for {{Reliability Research}}},
  author = {Koo, Terry K. and Li, Mae Y.},
  year = {2016},
  month = jun,
  journal = {Journal of Chiropractic Medicine},
  volume = {15},
  number = {2},
  pages = {155--163},
  issn = {1556-3707},
  doi = {10.1016/j.jcm.2016.02.012},
  abstract = {Objective Intraclass correlation coefficient (ICC) is a widely used reliability index in test-retest, intrarater, and interrater reliability analyses. This article introduces the basic concept of ICC in the content of reliability analysis. Discussion for Researchers There are 10 forms of ICCs. Because each form involves distinct assumptions in their calculation and will lead to different interpretations, researchers should explicitly specify the ICC form they used in their calculation. A thorough review of the research design is needed in selecting the appropriate form of ICC to evaluate reliability. The best practice of reporting ICC should include software information, ``model,'' ``type,'' and ``definition'' selections. Discussion for Readers When coming across an article that includes ICC, readers should first check whether information about the ICC form has been reported and if an appropriate ICC form was used. Based on the 95\% confident interval of the ICC estimate, values less than 0.5, between 0.5 and 0.75, between 0.75 and 0.9, and greater than 0.90 are indicative of poor, moderate, good, and excellent reliability, respectively. Conclusion This article provides a practical guideline for clinical researchers to choose the correct form of ICC and suggests the best practice of reporting ICC parameters in scientific publications. This article also gives readers an appreciation for what to look for when coming across ICC while reading an article.},
  pmcid = {PMC4913118},
  pmid = {27330520},
  file = {C\:\\Users\\peder\\Zotero\\storage\\XFJC5LAT\\Koo and Li - 2016 - A Guideline of Selecting and Reporting Intraclass .pdf}
}

@article{Lakens2018,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {259--269},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245918770963},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\JV9TM7S4\\Lakens et al. - 2018 - Equivalence Testing for Psychological Research A .pdf}
}

@article{Lakens2021a,
  title = {Improving {{Transparency}}, {{Falsifiability}}, and {{Rigor}} by {{Making Hypothesis Tests Machine}}-{{Readable}}},
  author = {Lakens, Dani{\"e}l and DeBruine, Lisa M.},
  year = {2021},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {2515245920970949},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920970949},
  abstract = {Making scientific information machine-readable greatly facilitates its reuse. Many scientific articles have the goal to test a hypothesis, so making the tests of statistical predictions easier to find and access could be very beneficial. We propose an approach that can be used to make hypothesis tests machine-readable. We believe there are two benefits to specifying a hypothesis test in such a way that a computer can evaluate whether the statistical prediction is corroborated or not. First, hypothesis tests become more transparent, falsifiable, and rigorous. Second, scientists benefit if information related to hypothesis tests in scientific articles is easily findable and reusable, for example, to perform meta-analyses, conduct peer review, and examine metascientific research questions. We examine what a machine-readable hypothesis test should look like and demonstrate the feasibility of machine-readable hypothesis tests in a real-life example using the fully operational prototype R package scienceverse.},
  language = {en},
  keywords = {hypothesis testing,machine readability,metadata,scholarly communication},
  file = {C\:\\Users\\peder\\Zotero\\storage\\DN6KCHLU\\Lakens and DeBruine - 2021 - Improving Transparency, Falsifiability, and Rigor .pdf}
}

@article{Li2014,
  title = {The Default Mode Network and Social Understanding of Others: What Do Brain Connectivity Studies Tell Us},
  shorttitle = {The Default Mode Network and Social Understanding of Others},
  author = {Li, Wanqing and Mai, Xiaoqin and Liu, Chao},
  year = {2014},
  journal = {Frontiers in Human Neuroscience},
  volume = {0},
  publisher = {{Frontiers}},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2014.00074},
  abstract = {The Default Mode Network (DMN) has been found to be involved in various domains of cognitive and social processing. The present article will review brain connectivity results related to the DMN in the fields of social understanding of others: emotion perception, empathy, theory of mind, and morality. Most of the reviewed studies focused on healthy subjects with no neurological and psychiatric disease, but some studies on patients with autism and psychopathy will also be discussed. Common results show that the medial prefrontal cortex (MPFC) plays a key role in the social understanding of others, and the subregions of the MPFC contribute differently to this function according to their roles in different subsystems of the DMN. At the bottom, the ventral MPFC in the medial temporal lobe subsystem and its connections with emotion regions are mainly associated with emotion engagement during social interactions. Above, the anterior MPFC (aMPFC) in the cortical midline structures and its connections with posterior and anterior cingulate cortex contribute mostly to making self-other distinctions. At the top, the dorsal MPFC (dMPFC) in the dMPFC subsystem and its connection with the temporo-parietal junction (TPJ) are primarily related to the understanding of other's mental states. As behaviors become more complex, the related regions in frontal cortex are located higher. This reflects the transfer of information processing from automatic to cognitive processes with the increase of the complexity of social interaction. Besides the MPFC and TPJ, the connectivities of posterior cingulate cortex also show some changes during tasks from the four social fields. These results indicate that the DMN is indispensable in the social understanding of others.},
  language = {English},
  keywords = {brain connectivity,Default Mode Network,Empathy,morality,social cognition,Theory of Mind},
  file = {C\:\\Users\\peder\\Zotero\\storage\\LPZNA4GT\\Li et al. - 2014 - The default mode network and social understanding .pdf}
}

@article{Makel2012,
  title = {Replications in {{Psychology Research}}: {{How Often Do They Really Occur}}?},
  shorttitle = {Replications in {{Psychology Research}}},
  author = {Makel, Matthew C. and Plucker, Jonathan A. and Hegarty, Boyd},
  year = {2012},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {537--542},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691612460688},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\P3UMEM3L\\Makel et al. - 2012 - Replications in Psychology Research How Often Do .pdf}
}

@article{Martin-Martin2018,
  title = {Google {{Scholar}}, {{Web}} of {{Science}}, and {{Scopus}}: {{A}} Systematic Comparison of Citations in 252 Subject Categories},
  shorttitle = {Google {{Scholar}}, {{Web}} of {{Science}}, and {{Scopus}}},
  author = {{Mart{\'i}n-Mart{\'i}n}, Alberto and {Orduna-Malea}, Enrique and Thelwall, Mike and {Delgado L{\'o}pez-C{\'o}zar}, Emilio},
  year = {2018},
  month = nov,
  journal = {Journal of Informetrics},
  volume = {12},
  number = {4},
  pages = {1160--1177},
  issn = {17511577},
  doi = {10.1016/j.joi.2018.09.002},
  language = {en}
}

@article{Matiasz2018,
  title = {{{ResearchMaps}}.Org for Integrating and Planning Research},
  author = {Matiasz, Nicholas J. and Wood, Justin and Doshi, Pranay and Speier, William and Beckemeyer, Barry and Wang, Wei and Hsu, William and Silva, Alcino J.},
  year = {2018},
  journal = {PloS One},
  volume = {13},
  number = {5},
  pages = {e0195271},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0195271},
  abstract = {To plan experiments, a biologist needs to evaluate a growing set of empirical findings and hypothetical assertions from diverse fields that use increasingly complex techniques. To address this problem, we operationalized principles (e.g., convergence and consistency) that biologists use to test causal relations and evaluate experimental evidence. With the framework we derived, we then created a free, open-source web application that allows biologists to create research maps, graph-based representations of empirical evidence and hypothetical assertions found in research articles, reviews, and other sources. With our ResearchMaps web application, biologists can systematically reason through the research that is most important to them, as well as evaluate and plan experiments with a breadth and precision that are unlikely without such a tool.},
  language = {eng},
  pmcid = {PMC5933701},
  pmid = {29723213},
  keywords = {Biology,Computer Graphics,Internet,Research}
}

@article{Meehl1990,
  title = {Appraising and {{Amending Theories}}: {{The Strategy}} of {{Lakatosian Defense}} and {{Two Principles}} That {{Warrant It}}},
  shorttitle = {Appraising and {{Amending Theories}}},
  author = {Meehl, Paul E.},
  year = {1990},
  month = apr,
  journal = {Psychological Inquiry},
  volume = {1},
  number = {2},
  pages = {108--141},
  issn = {1047-840X, 1532-7965},
  doi = {10.1207/s15327965pli0102_1},
  language = {en}
}

@misc{Muschelli2019,
  title = {Rscopus: {{Scopus Database}} '{{API}}' {{Interface}}},
  shorttitle = {Rscopus},
  author = {Muschelli, John},
  year = {2019},
  month = sep,
  abstract = {Uses Elsevier 'Scopus' API {$<$}https://dev.elsevier.com/sc\_apis.html{$>$} to download information about authors and their citations.},
  copyright = {GPL-2},
  keywords = {WebTechnologies}
}

@techreport{Nuijten2017,
  type = {Preprint},
  title = {The {{Validity}} of the {{Tool}} ``Statcheck'' in {{Discovering Statistical Reporting Inconsistencies}}},
  author = {Nuijten, Michele B. and {van Assen}, Marcel A. L. M. and Hartgerink, Chris Hubertus Joseph and Epskamp, Sacha and Wicherts, Jelte M.},
  year = {2017},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/tcxaj},
  abstract = {The R package ``statcheck'' (Epskamp \&amp; Nuijten, 2016)  is a tool to extract statistical results from articles and check whether the reported p-value matches the accompanying test statistic and degrees of freedom. A previous study showed high interrater reliabilities (between .76 and .89) between statcheck and manual coding of inconsistencies (.76 - .89; Nuijten, Hartgerink, Van Assen, Epskamp, \&amp; Wicherts, 2016). Here we present an additional, detailed study of the validity of statcheck. In Study 1, we calculated its sensitivity and specificity. We found that statcheck's sensitivity (true positive rate) and specificity (true negative rate) were high: between 85.3\% and 100\%, and between 96.0\% and 100\%, respectively, depending on the assumptions and settings. The overall accuracy of statcheck ranged from 96.2\% to 99.9\%. In Study 2, we investigated statcheck's ability to deal with statistical corrections for multiple testing or violations of assumptions in articles. We found that the prevalence of corrections for multiple testing or violations of assumptions in psychology was higher than we initially estimated in Nuijten et al. (2016). Although we found numerous reporting inconsistencies in results corrected for violations of the sphericity assumption, we demonstrate that inconsistencies associated with statistical corrections are not what is causing the high estimates of the prevalence of statistical reporting inconsistencies in psychology.},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\XFSW6BLS\\Nuijten et al. - 2017 - The Validity of the Tool “statcheck” in Discoverin.pdf}
}

@article{Orben2020,
  title = {Crud ({{Re}}){{Defined}}},
  author = {Orben, Amy and Lakens, Dani{\"e}l},
  year = {2020},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {2},
  pages = {238--247},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245920917961},
  abstract = {The idea that in behavioral research everything correlates with everything else was a niche area of the scientific literature for more than half a century. With the increasing availability of large data sets in psychology, the ``crud'' factor has, however, become more relevant than ever before. When referenced in empirical work, it is often used by researchers to discount minute\textemdash but statistically significant\textemdash effects that are deemed too small to be considered meaningful. This review tracks the history of the crud factor and examines how its use in the psychological- and behavioral-science literature has developed to this day. We highlight a common and deep-seated lack of understanding about what the crud factor is and discuss whether it can be proven to exist or estimated and how it should be interpreted. This lack of understanding makes the crud factor a convenient tool for psychologists to use to disregard unwanted results, even though the presence of a crud factor should be a large inconvenience for the discipline. To inspire a concerted effort to take the crud factor more seriously, we clarify the definitions of important concepts, highlight current pitfalls, and pose questions that need to be addressed to ultimately improve understanding of the crud factor. Such work will be necessary to develop the crud factor into a useful concept encouraging improved psychological research.},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\L7RLXRMI\\Orben and Lakens - 2020 - Crud (Re)Defined.pdf}
}

@article{Pittelkow2021,
  title = {The {{Process}} of {{Replication Target Selection}}: {{What}} to {{Consider}}?},
  shorttitle = {The {{Process}} of {{Replication Target Selection}}},
  author = {Pittelkow, Merle-Marie and Field, Sarahanne Miranda and Isager, Peder M. and van 't Veer, Anna and van Ravenzwaaij, Don},
  year = {2021},
  month = apr,
  publisher = {{OSF}},
  abstract = {Increased execution of replication studies contributes to the effort to restore credibility of empirical research. However, a second generation of problems arises: the number of potential replication targets is at a serious mismatch with available resources. Given limited resources, replication target selection should be well-justified, systematic, and transparently communicated. At present the discussion on what to consider when selecting a replication target is limited to theoretical discussion, self-reported justifications, and a few formalized suggestions. \textbackslash\textbackslash{} Here, we propose a study to involve the scientific community in creating a list of considerations generally regarded important by social scientists with regards to replication target selection. We will employ a modified Delphi approach. First, we constructed a preliminary list of considerations. Second, individuals who previously selected a replication target will be surveyed with regards to their considerations. Results from the survey will be incorporated into the preliminary list of considerations. Lastly, the updated list will be sent to a group of individuals, knowledgeable about concerns regarding replication target selection. Over the course of several rounds, we aim to establish consensus regarding what to consider when selecting a replication target. The proposal has been submitted as a Stage 1 Registered Report to Royal Society of Open Science      Hosted on the Open Science Framework},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\EP6TQ5U8\\j7ksu.html}
}

@article{Poldrack2017,
  title = {Scanning the Horizon: Towards Transparent and Reproducible Neuroimaging Research},
  shorttitle = {Scanning the Horizon},
  author = {Poldrack, Russell A. and Baker, Chris I. and Durnez, Joke and Gorgolewski, Krzysztof J. and Matthews, Paul M. and Munaf{\`o}, Marcus R. and Nichols, Thomas E. and Poline, Jean-Baptiste and Vul, Edward and Yarkoni, Tal},
  year = {2017},
  month = feb,
  journal = {Nature Reviews Neuroscience},
  volume = {18},
  number = {2},
  pages = {115--126},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn.2016.167},
  abstract = {Functional neuroimaging techniques have transformed our ability to probe the neurobiological basis of behaviour and are increasingly being applied by the wider neuroscience community. However, concerns have recently been raised that the conclusions that are drawn from some human neuroimaging studies are either spurious or not generalizable. Problems such as low statistical power, flexibility in data analysis, software errors and a lack of direct replication apply to many fields, but perhaps particularly to functional MRI. Here, we discuss these problems, outline current and suggested best practices, and describe how we think the field should evolve to produce the most meaningful and reliable answers to neuroscientific questions.},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\UEUG36RV\\Poldrack et al. - 2017 - Scanning the horizon towards transparent and repr.pdf}
}

@misc{Ram2017,
  title = {{{rAltmetric}}: {{Retrieves Altmerics Data}} for {{Any Published Paper}} from '{{Altmetric}}.Com'},
  shorttitle = {{{rAltmetric}}},
  author = {Ram, Karthik},
  year = {2017},
  month = apr,
  abstract = {Provides a programmatic interface to the citation information and alternate metrics provided by 'Altmetric'. Data from Altmetric allows researchers to immediately track the impact of their published work, without having to wait for citations. This allows for faster engagement with the audience interested in your work. For more information, visit {$<$}https://www.altmetric.com/{$>$}.},
  copyright = {MIT + file LICENSE}
}

@misc{Revelle2021,
  title = {Psych: {{Procedures}} for {{Psychological}}, {{Psychometric}}, and {{Personality Research}}},
  shorttitle = {Psych},
  author = {Revelle, William},
  year = {2021},
  month = jun,
  abstract = {A general purpose toolbox for personality, psychometric theory and experimental psychology. Functions are primarily for multivariate analysis and scale construction using factor analysis, principal component analysis, cluster analysis and reliability analysis, although others provide basic descriptive statistics. Item Response Theory is done using factor analysis of tetrachoric and polychoric correlations. Functions for analyzing data at multiple levels include within and between group statistics, including correlations and factor analysis. Functions for simulating and testing particular item and test structures are included. Several functions serve as a useful front end for structural equation modeling. Graphical displays of path diagrams, factor analysis and structural equation models are created using basic graphics. Some of the functions are written to support a book on psychometric theory as well as publications in personality research. For more information, see the {$<$}https://personality-project.org/r/{$>$} web page.},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {Psychometrics}
}

@article{Sullivan2012,
  title = {Using {{Effect Size}}\textemdash or {{Why}} the {{P Value Is Not Enough}}},
  author = {Sullivan, Gail M. and Feinn, Richard},
  year = {2012},
  month = sep,
  journal = {Journal of Graduate Medical Education},
  volume = {4},
  number = {3},
  pages = {279--282},
  issn = {1949-8349},
  doi = {10.4300/JGME-D-12-00156.1},
  abstract = {These statements about the importance of effect sizes were made by two of the most influential statistician-researchers of the past half-century. Yet many submissions to Journal of Graduate Medical Education omit mention of the effect size in quantitative studies while prominently displaying the P value. In this paper, we target readers with little or no statistical background in order to encourage you to improve your comprehension of the relevance of effect size for planning, analyzing, reporting, and understanding education research studies.},
  file = {C\:\\Users\\peder\\Zotero\\storage\\HH26SN57\\Sullivan and Feinn - 2012 - Using Effect Size—or Why the P Value Is Not Enough.pdf}
}

@article{Szpunar2014,
  title = {Repetition-Related Reductions in Neural Activity Reveal Component Processes of Mental Simulation},
  author = {Szpunar, Karl K. and St Jacques, Peggy L. and Robbins, Clifford A. and Wig, Gagan S. and Schacter, Daniel L.},
  year = {2014},
  month = may,
  journal = {Social Cognitive and Affective Neuroscience},
  volume = {9},
  number = {5},
  pages = {712--722},
  issn = {1749-5024},
  doi = {10.1093/scan/nst035},
  abstract = {In everyday life, people adaptively prepare for the future by simulating dynamic events about impending interactions with people, objects and locations. Previous research has consistently demonstrated that a distributed network of frontal-parietal-temporal brain regions supports this ubiquitous mental activity. Nonetheless, little is known about the manner in which specific regions of this network contribute to component features of future simulation. In two experiments, we used a functional magnetic resonance (fMR)-repetition suppression paradigm to demonstrate that distinct frontal-parietal-temporal regions are sensitive to processing the scenarios or what participants imagined was happening in an event (e.g., medial prefrontal, posterior cingulate, temporal-parietal and middle temporal cortices are sensitive to the scenarios associated with future social events), people (medial prefrontal cortex), objects (inferior frontal and premotor cortices) and locations (posterior cingulate/retrosplenial, parahippocampal and posterior parietal cortices) that typically constitute simulations of personal future events. This pattern of results demonstrates that the neural substrates of these component features of event simulations can be reliably identified in the context of a task that requires participants to simulate complex, everyday future experiences.},
  language = {eng},
  pmcid = {PMC4014108},
  pmid = {23482621},
  keywords = {Adaptation; Physiological,Brain,Brain Mapping,default network,Female,fMRI,future event simulation,Humans,Imagination,Interpersonal Relations,Magnetic Resonance Imaging,Male,Neural Pathways,Neuropsychological Tests,repetition suppression,Young Adult},
  file = {C\:\\Users\\peder\\Zotero\\storage\\VSVAA8WA\\Szpunar et al. - 2014 - Repetition-related reductions in neural activity r.pdf}
}

@article{Szucs2017,
  title = {Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = {2017},
  month = mar,
  journal = {PLOS Biology},
  volume = {15},
  number = {3},
  pages = {e2000797},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2000797},
  abstract = {We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64\textendash 1.46) for nominally statistically significant results and D = 0.24 (0.11\textendash 0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50\% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.},
  language = {en},
  keywords = {Behavioral neuroscience,Cognitive neuroscience,Cognitive psychology,Experimental psychology,Medical journals,Scientific publishing,Statistical data,Statistical distributions},
  file = {C\:\\Users\\peder\\Zotero\\storage\\DQ8QEYXQ\\Szucs and Ioannidis - 2017 - Empirical assessment of published effect sizes and.pdf;C\:\\Users\\peder\\Zotero\\storage\\FIYF44DM\\article.html}
}

@article{Tamir2012,
  title = {Disclosing Information about the Self Is Intrinsically Rewarding},
  author = {Tamir, D. I. and Mitchell, J. P.},
  year = {2012},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {21},
  pages = {8038--8043},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1202129109},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\Y3RSKYRE\\Tamir and Mitchell - 2012 - Disclosing information about the self is intrinsic.pdf}
}

@misc{Tay2020,
  title = {Why Openly Available Abstracts Are Important - Overview of the Current State of Affairs},
  author = {Tay, Aaron and Kramer, Bianca and Waltman, Ludo},
  year = {2020},
  month = jun,
  journal = {Leiden Madtrics},
  abstract = {Openness of the metadata of scientific articles is increasingly being discussed. In this blog post, Aaron Tay (SMU Libraries, Singapore Management University), Bianca Kramer (Utrecht University Library), and Ludo Waltman (CWTS, Leiden University) discuss the value of openly available abstracts.},
  howpublished = {https://www.leidenmadtrics.nl/articles/why-openly-available-abstracts-are-important-overview-of-the-current-state-of-affairs},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\5FJ5SAP4\\why-openly-available-abstracts-are-important-overview-of-the-current-state-of-affairs.html}
}

@article{VanEck2010,
  title = {Software Survey: {{VOSviewer}}, a Computer Program for Bibliometric Mapping},
  shorttitle = {Software Survey},
  author = {{van Eck}, Nees Jan and Waltman, Ludo},
  year = {2010},
  journal = {Scientometrics},
  volume = {2},
  number = {84},
  pages = {523--538},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-009-0146-3},
  abstract = {We present VOSviewer, a freely available computer program that we have developed for constructing and viewing bibliometric maps. Unlike most computer programs that are used for bibliometric mapping, VOSviewer pays special attention to the graphical representation of bibliometric maps. The functionality of VOSviewer is especially useful for displaying large bibliometric maps in an easy-to-interpret way. The paper consists of three parts. In the first part, an overview of VOSviewer's functionality for displaying bibliometric maps is provided. In the second part, the technical implementation of specific parts of the program is discussed. Finally, in the third part, VOSviewer's ability to handle large maps is demonstrated by using the program to construct and display a co-citation map of 5,000 major scientific journals.},
  language = {English},
  file = {C\:\\Users\\peder\\Zotero\\storage\\DVCL9PYT\\Eck and Waltman - 2010 - Software survey VOSviewer, a computer program for.pdf;C\:\\Users\\peder\\Zotero\\storage\\NFU57Q6W\\bwmeta1.element.html}
}

@incollection{VanEck2014,
  title = {Visualizing {{Bibliometric Networks}}},
  booktitle = {Measuring {{Scholarly Impact}}: {{Methods}} and {{Practice}}},
  author = {{van Eck}, Nees Jan and Waltman, Ludo},
  editor = {Ding, Ying and Rousseau, Ronald and Wolfram, Dietmar},
  year = {2014},
  pages = {285--320},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-10377-8_13},
  abstract = {This chapter provides an introduction to the topic of visualizing bibliometric networks. First, the most commonly studied types of bibliometric networks (i.e., citation, co-citation, bibliographic coupling, keyword co-occurrence, and coauthorship networks) are discussed, and three popular visualization approaches (i.e., distance-based, graph-based, and timeline-based approaches) are distinguished. Next, an overview is given of a number of software tools that can be used for visualizing bibliometric networks. In the second part of the chapter, the focus is specifically on two software tools: VOSviewer and CitNetExplorer. The techniques used by these tools to construct, analyze, and visualize bibliometric networks are discussed. In addition, tutorials are offered that demonstrate in a step-by-step manner how both tools can be used. Finally, the chapter concludes with a discussion of the limitations and the proper use of bibliometric network visualizations and with a summary of some ongoing and future developments.},
  isbn = {978-3-319-10377-8},
  language = {en},
  keywords = {Bibliographic Coupling,Bibliographic Data,Citation Network,Citation Relation,Noun Phrase},
  file = {C\:\\Users\\peder\\Zotero\\storage\\FFABI7UD\\van Eck and Waltman - 2014 - Visualizing Bibliometric Networks.pdf}
}

@article{Waltman2011,
  title = {Towards a New Crown Indicator: {{Some}} Theoretical Considerations},
  shorttitle = {Towards a New Crown Indicator},
  author = {Waltman, Ludo and {van Eck}, Nees Jan and {van Leeuwen}, Thed N. and Visser, Martijn S. and {van Raan}, Anthony F. J.},
  year = {2011},
  month = jan,
  journal = {Journal of Informetrics},
  volume = {5},
  number = {1},
  pages = {37--47},
  issn = {1751-1577},
  doi = {10.1016/j.joi.2010.08.001},
  abstract = {The crown indicator is a well-known bibliometric indicator of research performance developed by our institute. The indicator aims to normalize citation counts for differences among fields. We critically examine the theoretical basis of the normalization mechanism applied in the crown indicator. We also make a comparison with an alternative normalization mechanism. The alternative mechanism turns out to have more satisfactory properties than the mechanism applied in the crown indicator. In particular, the alternative mechanism has a so-called consistency property. The mechanism applied in the crown indicator lacks this important property. As a consequence of our findings, we are currently moving towards a new crown indicator, which relies on the alternative normalization mechanism.},
  language = {en},
  keywords = {Bibliometric indicator,Consistency,Crown indicator,Normalization},
  file = {C\:\\Users\\peder\\Zotero\\storage\\KFII24P4\\Waltman et al. - 2011 - Towards a new crown indicator Some theoretical co.pdf;C\:\\Users\\peder\\Zotero\\storage\\6MY6VJIV\\S1751157710000817.html}
}

@incollection{Waltman2019,
  title = {Field {{Normalization}} of {{Scientometric Indicators}}},
  booktitle = {Springer {{Handbook}} of {{Science}} and {{Technology Indicators}}},
  author = {Waltman, Ludo and {van Eck}, Nees Jan},
  editor = {Gl{\"a}nzel, Wolfgang and Moed, Henk F. and Schmoch, Ulrich and Thelwall, Mike},
  year = {2019},
  series = {Springer {{Handbooks}}},
  pages = {281--300},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-02511-3_11},
  abstract = {When scientometric indicators are used to compare research units active in different scientific fields, there is often a need to make corrections for differences between fields, for instance, differences in publication, collaboration, and citation practices. Field-normalized indicators aim to make such corrections. The design of these indicators is a significant challenge. We discuss the main issues in the design of field-normalized indicators and present an overview of the different approaches that have been developed for dealing with the problem of field normalization. We also discuss how field-normalized indicators can be evaluated and consider the sensitivity of scientometric analyses to the choice of a field-normalization approach.},
  isbn = {978-3-030-02511-3},
  language = {en},
  keywords = {field classification system,field normalization,impact indicator,productivity indicator,scientometric indicator,scientometrics},
  file = {C\:\\Users\\peder\\Zotero\\storage\\8AWTNXGW\\Waltman and van Eck - 2019 - Field Normalization of Scientometric Indicators.pdf}
}

@article{Westfall2014,
  title = {Statistical Power and Optimal Design in Experiments in Which Samples of Participants Respond to Samples of Stimuli.},
  author = {Westfall, Jacob and Kenny, David A. and Judd, Charles M.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {5},
  pages = {2020--2045},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/xge0000014},
  language = {en}
}

@misc{zotero-3943,
  title = {Web of {{Science Core Collection Help}}},
  copyright = {Clarivate Analytics},
  howpublished = {https://web.archive.org/web/20210525004218/https://images.webofknowledge.com/images/help/WOS/hp\_subject\_category\_terms\_tasca.html},
  file = {C\:\\Users\\peder\\Zotero\\storage\\E7ZXKT4P\\hp_subject_category_terms_tasca.html}
}

@article{Zwaan2018,
  title = {Making Replication Mainstream},
  author = {Zwaan, Rolf A. and Etz, Alexander and Lucas, Richard E. and Donnellan, M. Brent},
  year = {2018},
  journal = {Behavioral and Brain Sciences},
  volume = {41},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X17001972},
  language = {en},
  file = {C\:\\Users\\peder\\Zotero\\storage\\8BNHK92K\\Zwaan et al. - 2018 - Making replication mainstream.pdf}
}

@article{lakens_improving_2021,
  title = {Improving {{Transparency}}, {{Falsifiability}}, and {{Rigor}} by {{Making Hypothesis Tests Machine-Readable}}},
  author = {Lakens, Dani\"el and DeBruine, Lisa M.},
  date = {2021-04-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {2515245920970949},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245920970949},
  url = {https://doi.org/10.1177/2515245920970949},
  urldate = {2021-07-13},
  abstract = {Making scientific information machine-readable greatly facilitates its reuse. Many scientific articles have the goal to test a hypothesis, so making the tests of statistical predictions easier to find and access could be very beneficial. We propose an approach that can be used to make hypothesis tests machine-readable. We believe there are two benefits to specifying a hypothesis test in such a way that a computer can evaluate whether the statistical prediction is corroborated or not. First, hypothesis tests become more transparent, falsifiable, and rigorous. Second, scientists benefit if information related to hypothesis tests in scientific articles is easily findable and reusable, for example, to perform meta-analyses, conduct peer review, and examine metascientific research questions. We examine what a machine-readable hypothesis test should look like and demonstrate the feasibility of machine-readable hypothesis tests in a real-life example using the fully operational prototype R package scienceverse.},
  langid = {english},
  keywords = {hypothesis testing,machine readability,metadata,scholarly communication},
  file = {C\:\\Users\\dlakens\\Zotero\\storage\\D4MMJJIM\\Lakens_DeBruine_2021_Improving Transparency, Falsifiability, and Rigor by Making Hypothesis Tests.pdf}
}

@article{pittelkow_process_2023,
  title = {The Process of Replication Target Selection in Psychology: What to Consider?},
  shorttitle = {The Process of Replication Target Selection in Psychology},
  author = {Pittelkow, Merle-Marie and Field, Sarahanne M. and Isager, Peder M. and van 't Veer, Anna E. and Anderson, Thomas and Cole, Scott N. and Dominik, Tom\'a\v{s} and Giner-Sorolla, Roger and Gok, Sebahat and Heyman, Tom and Jekel, Marc and Luke, Timothy J. and Mitchell, David B. and Peels, Rik and Pendrous, Rosina and Sarrazin, Samuel and Schauer, Jacob M. and Specker, Eva and Tran, Ulrich S. and Vranka, Marek A. and Wicherts, Jelte M. and Yoshimura, Naoto and Zwaan, Rolf A. and van Ravenzwaaij, Don},
  options = {useprefix=true},
  date = {2023-02},
  journaltitle = {Royal Society Open Science},
  volume = {10},
  number = {2},
  pages = {210586},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.210586},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.210586},
  urldate = {2023-02-26},
  abstract = {Increased execution of replication studies contributes to the effort to restore credibility of empirical research. However, a second generation of problems arises: the number of potential replication targets is at a serious mismatch with available resources. Given limited resources, replication target selection should be well-justified, systematic and transparently communicated. At present the discussion on what to consider when selecting a replication target is limited to theoretical discussion, self-reported justifications and a few formalized suggestions. In this Registered Report, we proposed a study involving the scientific community to create a list of considerations for consultation when selecting a replication target in psychology. We employed a modified Delphi approach. First, we constructed a preliminary list of considerations. Second, we surveyed psychologists who previously selected a replication target with regards to their considerations. Third, we incorporated the results into the preliminary list of considerations and sent the updated list to a group of individuals knowledgeable about concerns regarding replication target selection. Over the course of several rounds, we established consensus regarding what to consider when selecting a replication target. The resulting checklist can be used for transparently communicating the rationale for selecting studies for replication.},
  keywords = {consensus,replication,study selection},
  file = {C\:\\Users\\dlakens\\Zotero\\storage\\K6IHK8BL\\Pittelkow et al. - 2023 - The process of replication target selection in psy.pdf}
}


@article{appelbaum_journal_2018,
  title = {Journal Article Reporting Standards for Quantitative Research in Psychology: {{The APA Publications}} and {{Communications Board}} Task Force Report.},
  shorttitle = {Journal Article Reporting Standards for Quantitative Research in Psychology},
  author = {Appelbaum, Mark and Cooper, Harris and Kline, Rex B. and Mayo-Wilson, Evan and Nezu, Arthur M. and Rao, Stephen M.},
  date = {2018-01-18},
  journaltitle = {American Psychologist},
  volume = {73},
  number = {1},
  pages = {3--25},
  publisher = {{US: American Psychological Association}},
  issn = {1935-990X},
  doi = {10.1037/amp0000191},
  url = {https://psycnet.apa.org/fulltext/2018-00750-002.pdf},
  urldate = {2020-07-25}
}

@article{gottfredson_evaluating_1978,
  title = {Evaluating Psychological Research Reports: {{Dimensions}}, Reliability, and Correlates of Quality Judgments},
  shorttitle = {Evaluating Psychological Research Reports},
  author = {Gottfredson, Stephen D.},
  date = {1978},
  journaltitle = {American Psychologist},
  volume = {33},
  pages = {920--934},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1935-990X},
  doi = {10.1037/0003-066X.33.10.920},
  abstract = {Presents a series of studies designed to investigate 3 major aspects of the peer-evaluation system. 299 editors and editorial consultants for 9 major psychology journals were surveyed for opinions about the desirability of article characteristics. Dimensional structures for evaluation were explored, resulting in a set of prescriptive norms for assessment. Substantial agreement on the desirability of article characteristics was demonstrated, and psychologists heavily involved in the manuscript decision-making processes associated with different journals apparently employed these dimensions in the same way. These results were used in a 2nd study demonstrating increased reliability of peer judgments of article quality. 540 "experts," nominated by article authors, evaluated 387 articles that had appeared in the 9 journals. Finally, a 3rd study found that peer judgments of article quality and impact were only very modestly correlated with subsequent citation (Science Citation Index) of the articles. (53 ref) (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Evaluation,Scientific Communication},
  file = {C\:\\Users\\dlakens\\Zotero\\storage\\I5YP9KNB\\1979-24404-001.html}
}

@article{scheel_why_2021,
  title = {Why {{Hypothesis Testers Should Spend Less Time Testing Hypotheses}}},
  author = {Scheel, Anne M. and Tiokhin, Leonid and Isager, Peder M. and Lakens, Dani\"el},
  date = {2021-07-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {16},
  number = {4},
  pages = {744--755},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620966795},
  url = {https://doi.org/10.1177/1745691620966795},
  urldate = {2022-01-10},
  abstract = {For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance tests made research on theories in the social sciences basically uninterpretable. In response to the replication crisis, reforms in psychology have focused on formalizing procedures for testing hypotheses. These reforms were necessary and influential. However, as an unexpected consequence, psychological scientists have begun to realize that they may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established a sound ``derivation chain'' between test and theory is counterproductive. Instead, various nonconfirmatory research activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses, researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships between concepts and the functional form of those relationships, and identifying boundary conditions and auxiliary assumptions. Providing these inputs should be recognized and incentivized as a crucial goal in itself. In this article, we discuss how shifting the focus to nonconfirmatory research can tie together many loose ends of psychology's reform movement and help us to develop strong, testable theories, as Paul Meehl urged.},
  langid = {english},
  keywords = {exploratory research,hypothesis testing,replication crisis},
  file = {C\:\\Users\\dlakens\\Zotero\\storage\\ZA6VWUJH\\Scheel et al. - 2021 - Why Hypothesis Testers Should Spend Less Time Test.pdf}
}

@article{schmidt_shall_2009,
  title = {Shall We Really Do It Again? {{The}} Powerful Concept of Replication Is Neglected in the Social Sciences.},
  shorttitle = {Shall We Really Do It Again?},
  author = {Schmidt, Stefan},
  date = {2009},
  journaltitle = {Review of General Psychology},
  volume = {13},
  number = {2},
  pages = {90--100},
  issn = {1939-1552, 1089-2680},
  doi = {10.1037/a0015108},
  url = {https://doi.org/10.1037/a0015108},
  urldate = {2015-11-30},
  langid = {english},
  annotation = {00000},
  file = {C\:\\Users\\dlakens\\Zotero\\storage\\VH2QG62Q\\Schmidt - 2009 - Shall we really do it again.pdf;C\:\\Users\\dlakens\\Zotero\\storage\\WEVWJKWV\\Schmidt - 2009 -.pdf}
}
