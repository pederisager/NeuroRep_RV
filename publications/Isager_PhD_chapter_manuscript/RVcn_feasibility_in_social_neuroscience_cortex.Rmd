---
title             : "Selecting Studies for Replication in Social Neuroscience: Exploring a Formal Approach"
shorttitle        : "Selecting Studies for Replication in Social Neuroscience"

author: 
  - name          : "Peder Mortvedt Isager"
    affiliation   : "1"
    role:
      - Conceptualization
      - Investigation
      - Visualization
      - Writing - Original Draft
      - Writing - Review & Editing
  - name          : "Daniël Lakens"
    affiliation   : "2"
    role:
      - Funding acquisition
      - Supervision
      - Writing - Review & Editing
  - name          : "Thed van Leeuwen"
    affiliation   : "3"
    role:
      - Data curation
      - Investigation
      - Resources
  - name          : "Anna E. van ‘t Veer"
    affiliation   : "4"
    corresponding : yes    # Define only one corresponding author
    address       : "Wassenaarseweg 52, 2333 AK Leiden, Leiden University, Institute of Psychology, Methodology and Statistics unit."
    email         : "a.e.van.t.veer@fsw.leidenuniv.nl"
    role:
      - Conceptualization
      - Supervision
      - Investigation
      - Writing - Review & Editing


affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Oslo New University College"
  - id            : "2"
    institution   : "Department of Industrial Engineering & Innovation Sciences, Eindhoven University of Technology"
  - id            : "3"
    institution   : "Centre for Science and Technology Studies, Leiden University"    
  - id            : "4"
    institution   : "Methodology and Statistics unit, Institute of Psychology, Leiden University"

authornote: |
  This work was funded by VIDI grant 452-17-013. We want to thank all student assistants from Leiden University and Eindhoven University of Technology who contributed to data collection for this project at one point or another. All data to reproduce this manuscript can be found at https://github.com/pederisager/NeuroRep_RV. 

abstract: |
 Replication of published results is crucial for ensuring the robustness and self-correction of research, yet replications are scarce in many fields. Replicating researchers will therefore often have to decide which of several relevant candidates to target for replication. Formal strategies for efficient study selection have been proposed, but none have been explored for practical feasibility–a prerequisite for validation. Here we move one step closer to efficient replication study selection by exploring the feasibility of a particular selection strategy that estimates *replication value* as a function of citation impact and sample size [@Isager2021] in the field of social neuroscience, where replication seems especially important. We first report our efforts to generate a representative candidate set of replication targets in social fMRI research. We then explore the feasibility and reliability of estimating replication value for the targets in our set, resulting in a dataset of 1358 studies ranked on their value of prioritising them for replication. In addition, we carefully examine possible measures, test auxiliary assumptions, and identify boundary conditions of measuring value and uncertainty. Our demonstrates the importance of how to implement study selection strategies in practice, and provides a general framework for exploring the feasibility of formal study selection strategies. We end our report by discussing how future validation studies might be designed.
  
keywords          : "replication value, replication, social neuroscience, bibliometric analysis, expected utility, exploratory report"
wordcount         : "9209"

bibliography      : ["neurorep_what_to_replicate.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf

header-includes   :
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
- \captionsetup[table]{labelformat=empty}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \lhead{ISAGER ET AL.}
- \rhead{}
---

```{r setup, include = FALSE, echo=FALSE, warning=FALSE}
options(digits = 3)

library("papaja")
library(tidyverse)
library(gridExtra)
library(kableExtra)
library(knitr)

getwd()
# Should yield something like "C:/Users/dlakens/surfdrive/R/NeuroRep_RV/publications/Isager_PhD_chapter_manuscript"

source("OSF_files/analysis/analyses_cortex.R")
data.raw <- readRDS("OSF_files/raw_data/articles.Rds")
data.all.raw <- readRDS("OSF_files/raw_data/studies.Rds")
```

# Introduction

Close replication of original research results is essential to increase our confidence that empirical findings are reliable [@schmidt_shall_2009]. When a study procedure is repeated in a new sample (preferably by a novel research team), spurious data patterns generated by random chance, mistakes, fraud, or questionable research practices can be detected and discarded. Close replication reduces research waste by preventing researchers from building on seemingly promising findings that are not true. This is especially important in social and cognitive neuroscience. Research in these fields are often vulnerable to inflated false positive rates and overestimated of effect sizes due to a combination of (1) low statistical power [@Szucs2017], (2) substantial researcher degrees of freedom that inflate the Type 1 error rate [@Carp2012; @Botvinik-Nezer2020], and (3) incentives to publish statistically significant results [@Button2013]. Unsurprisingly, rates of successful replications are low [@Boekel2015]. In spite of this, close replications of original studies are not common practice [@Poldrack2017; @Huber2019; @Ashar2021]. At the same time, the cost of data collection is high [@Poldrack2017], which leads to a conundrum. On the one hand, high data collection costs make it all the more important to conduct close replications and prevent costly studies from being built on spurious findings. . On the other hand, high costs limit how often replication studies can be conducted. With high limited resources and many non-replicated studies to choose from, researchers in social and cognitive neuroscience should consider which studies in the published literature would be the most important to replicate, so that resources directed towards replication can be spent optimally. 

Various formal strategies for replication study selection have been developed in recent years [@Field2019; @Matiasz2018; @Isager2021; supplementary formula documents in @Isager2020]. If effective, such strategies have great potential to increase the transparency and efficiency of replication study selection in neuroimaging research. When criteria for study selection are made transparent, it becomes easier to discuss which replication studies are most important to fund, conduct, and publish. Additionally, formal strategies allow researchers who agree on criteria to more easily identify and coordinate replication of the most important-to-replicate studies in their field. By increasing the efficiency of coordination and resource spending in replication research, formal study selection strategies present a major step forward towards the important goal of making replication part of mainstream research practice. 

However, no formal study selection strategies have been tested for application in social neuroscience. To be applicable, a strategy must meet two basic conditions. First, it must be feasible to apply the strategy in practice. That is, the information needed to execute the strategy must be possible to obtain given reasonable time and resource constraints. Most formal study selection strategies are based on a combination of statistical, bibliometric, and substantive information about the candidate studies, which is often not easy to access [e.g., @Tay2020; @Sullivan2012; @Furukawa2006; @Glasziou2008; @Federer2018]. The feasibility of existing strategies for application in any particular area of research is therefore uncertain. Second, provided that the strategy is feasible to apply we must validate that the strategy is actually helping us reach our prespecified research goals. All feasible selection strategies lead to *some* prioritization of studies, but whether this prioritization has any validity and practical utility is an empirical question. 

In this article we explore how feasible it is to apply a particular replication study selection strategy [@Isager2021] to fMRI research in social neuroscience. We focus on fMRI research in social neuroscience because replication studies in this field are both scarce and costly, and because this is the field of expertise of the first and last author. We reflect on the generalizability of our conclusions to other research areas in the general discussion. Before we are ready to compare different study selection strategies that have been proposed, we believe it is important to lay the ground work for such a comparison. After conceptualizing citation count as an indicator of the value of a study, and sample size as an indicator of uncertainty [@Isager2021] here we take the next step by examining the following elements (based on [@scheel_why_2021] that are required to make a future comparison of study selection strategies more meaningful. First, we explore different measures of citation impact, and examine if the source of the citation information matters for the rank-order. Second, after conceptually analyzing the relationships between variables in [@Isager2021], we empirically examine the relationship between different ways that the value and impact of studies can be operationalized by their citation count and sample size. Third, we identify examples of boundary conditions where the value of a study and the uncertainty about a study are not captured by the citation count and sample size, based on an in-depth evaluation process of a subset of replication candidates. Finally, we examine three auxiliary assumptions regarding 1) the possibility to control for the age of the article when computing the replication value based on the citation count, 2) that the source of the citation counts does not substantially impact the rank-order, and 3) that past citation counts predict future citation counts. 

We perform these four steps through a feasibility study, where we also examine if it is possible to code the required information to quantify uncertainty. We will explore whether the steps of a study selection strategy can be carried out in practice given reasonable time- and resource constraints. We end with a brief qualitative assessment of the studies recommended to us by the strategy, noting potential issues that future validation studies should examine more carefully. The primary goal in this article is to understand how best to implement the strategy of [@Isager2021] in social fMRI research so that the strategy can be validated for use in this field and potentially used for efficient study selection by future researchers. A secondary goal of this article is to curate a comprehensive set of empirical studies in the social fMRI literature that could form the basis for any replication study selection in this field, regardless of the selection strategy utilized. We summarize information about this set of studies, such as their citation counts and sample sizes, descriptively. By exploring information relevant to replication study selection in social fMRI we hope to leave researchers in this field better equipped to make well-informed decisions about which original research to prioritize for replication. 

# A four step approach to select studies for replication

To decide on a method for replication study selection, we must first settle on a goal, and a rationale for why selecting certain studies helps us reach this goal more efficiently. We here adopt the formal decision model for replication study selection proposed by @Isager2020. According to this model, the goal of a replication effort is to maximize the *expected utility* of knowledge gained. *Expected utility gain* can be approximated by the *replication value* of the target claim we want to replicate. In this model replication value is a function of the *value* (or importance) of having accurate knowledge about the target claim, and our *uncertainty* about the truth status of the claim based on available evidence prior to replicating. Research claims that are highly valuable or important, and about which we are highly uncertain, will have a high replication value, and should be prioritized for replication in order to maximize expected utility gain.

We have previously proposed an operational definition of replication value [@Isager2021], in which value is operationalized as the average yearly citation impact of the article in which a claim is reported, and uncertainty is operationalized as the sample size used to investigate the claim. Replication value is then operationalized as the indicator *RV~Cn~*:

\begin{equation} 
  \tag{1}
  RV_{Cn} = value\times uncertainty = \frac{w(C_{S})}{Y+1}\times\frac{1}{\sqrt{n}}
  (\#eq:1)
\end{equation}

where $RV~Cn~$ denotes a particular operationalization of replication value, $C$ stands for citation impact, $n$ stands for the total number of participants included in the study, $w()$ stands for the weighting function that should be applied to the citation impact (such as removing self-citations or not), $s$ denotes the source the citation data is retrieved from, and $Y$ stands for the age of the article in years. The equation assumes that average yearly citation impact is causally influenced by scientific impact, and that scientific impact partly determines  the value of a claim. It also assumes that sample size partly determines the standard error of estimates in a study, which in turn partly determines the uncertainty about claims studied. Although both the average citation per year and the sample size are imperfect measures of value and uncertainty, our auxiliary assumption is that they are sufficiently correlated with value and uncertainty to generate a useful rank order of replication value. 

We propose a four-step procedure for replication study selection based on *RV~Cn~* (see Figure 1). In the first step an initial set of candidate studies is identified based on the research interests and resource constraints of the replicating researcher. As with every systematic review of the literature, the scope needs to be broad enough to encompass all claims of interest to the researchers, but narrow enough so that the review process remains feasible. In the second step *RV~Cn~* is calculated for each study included in the set to create an initial estimate of rank-order expected utility gain. In the third step a subset of the studies with the highest *RV~Cn~* is inspected in-depth by reading the article. This step functions as an additional check of the *RV~Cn~* estimates, and has as the primary goal to evaluate additional factors relevant to the value and the uncertainty of the reported effect [e.g., @Field2019; @KNAW2018; @Heirene2021]. In this step researchers can also evaluate the feasibility of a replication study given the resources they have available, and the extent to which a replication study will be able to reduce uncertainty about the effect [@Isager2020]. In the fourth step the candidate deemed most worthwhile to replicate is selected. Alternatively, if the researcher thinks the subset of studies that has been inspected contains no candidate that is worth replicating or feasible to replicate, step 3 and 4 can be repeated for a second subset of studies.

![**Figure 1.** General study selection procedure in which the *RV~Cn~* indicator is implemented.](Figure_1.png){width=50%}

# Exploring the feasibility of using *RV~Cn~* for study selection in Social Neuroscience

*RV~Cn~* was developed to enable a more efficient coordination of replication efforts in social fMRI research. However, it is not clear whether it is feasible to use *RV~Cn~* in practice for study selection in a research area such as social fMRI research. Our exploration focuses on the first two steps of the four-step procedure listed in Figure 1. We report the results of our attempt to implement these steps in practice, including our method for collecting a sample set of replication candidates (step 1), and more importantly, our method for collecting the citation impact and sample size data necessary to calculate *RV~Cn~*, the reliability of our methods for generating accurate measures of citation counts and sample sizes, and describe the distribution of *RV~Cn~* for our set of candidates (step 2). In supplementary materials we also summarize our unsuccessful pilot efforts to collect additional quantitative information related to the *main finding* for each candidate studies in our set. Where a main finding could often be identified based on the abstract, it proved too difficult to identify which statistical test was the basis of this main finding. Finally, we also provide a brief qualitative evaluation of the recommendations produced by *RV~Cn~* to better understand what sort of studies are being recommended, what the boundary conditions of this study selection strategy are, and to understand the factors one might want to evaluate in when implementing step 3. We conclude the article by generating hypotheses for studies that could be undertaken to test the validity of *RV~Cn~*. 

## Step 1 - Determining an initial set of candidate studies

### Eligibility criteria

To test the feasibility of calculating *RV~Cn~* we first set out to determine a suitable set of candidate articles. This step is similar to any systematic literature review (e.g., a meta-analysis). We restricted our search for studies to fMRI research within social neuroscience between 2009-2019 at the time this decision was made. Although there is no need to restrict study selection to a specific time period, we reasoned that researchers might be especially interested in conducting replications of studies within a relatively recent time window to prevent unproductive follow-up research (when the original research is non-replicable) or stimulate follow-up research (when the original research is replicable).

### Search strategy

We used the Web of Science (WoS; www.webofknowledge.com) database to construct our candidate dataset. WoS does not have a predefined field category for social neuroscience. To identify articles related to social neuroscience, we implemented a two-pronged search strategy on 2019-02-21. . We first identified four journals in the WoS database as social neuroscience journals (Social Cognitive and Affective Neuroscience; Social Neuroscience; Behavioral Neuroscience; and Socioaffective Neuroscience Psychology). Empirical articles published in these journals were identified by submitting the following search term to Web of Science:

>(SO=(social neuroscience OR social cognitive and affective neuroscience OR behavioral neuroscience OR socioaffective neuroscience psychology) AND PY=( 2019 OR 2009 OR 2018 OR 2017 OR 2016 OR 2015 OR 2014 OR 2013 OR 2012 OR 2011 OR 2010 )) AND DOCUMENT TYPES: (Article)
	Timespan: 2009-2019. Indexes: SCI-EXPANDED, SSCI, A&HCI, CPCI-S, CPCI-SSH, ESCI.
	
To identify social neuroscience articles in general topic journals we searched the entire WoS database for articles containing the keywords “social” and “fMRI” in either the title or the abstract. Empirical articles containing the relevant keyword information were identified by submitting the following search term to WoS:

>ALL FIELDS: (fmri AND social)
Refined by: DOCUMENT TYPES: ( ARTICLE )
Timespan: 2009-2019. Indexes: SCI-EXPANDED, SSCI, A&HCI, CPCI-S, CPCI-SSH, ESCI.

`r sum(data.raw$source=="journal")` records were identified via this search strategy. 

### Selection process

The two search strategies yielded overlapping results. After removing duplicate records, the two search strategies yielded `r length(unique(data.raw$DI))` unique empirical articles in total (see Figure 2). Basic bibliometric information about each article, including author-provided keywords, were downloaded for all articles.

Authors PMI and AvtV reviewed the initial set of articles and excluded articles they did not believe would be feasible to replicate given their expertise and available resources, which meant excluding animal model research, highly invasive study designs, imaging methods outside our area of expertise, research on patient groups, and other keywords signaling the study would require highly specific samples, procedures, or technologies to perform. At this stage, exclusion criteria were not predetermined, but were exploratorily derived through inspecting keyword information in our initial candidate set. To ensure transparency a written record of the decision rationale for each excluded keyword has been made openly available on OSF (https://osf.io/mtx72/). Our final set of candidates contained `r nrow(data.bib)` empirical articles.

![**Figure 2.** Overview of candidate selection process and data points available for each respective analysis reported below.](sample_selection_process.png)

### Exploration of sample representativeness

Once the final set of candidate records was determined, we explored the available bibliographic information to ensure that the sample indeed seemed representative of the field of social fMRI research. The full dataset, including all bibliometric variables and a variable codebook, are available on OSF (https://osf.io/f7zdq/). The articles included in our dataset were published in `r n.journals` unique journals, consistent with our expectation that social neuroscience is a broad and loosely connected discipline of researchers from many subfields, who publish in a variety of specialty- and general-topic journals. Table 1 displays the name and frequency of the 20 journals most frequently published in (`r perc.articlesintopjournals` of all articles in the set were published in these 20 journals). 

```{r tab1}
apa_table(jou.freq, caption = "\\textbf{Table 1:} Journals which the articles in our initial candidate set were most frequently published in.", escape = F)
```

We used the statistical visualization software VOSviewer [@VanEck2010] to extract commonly mentioned terms from the titles and abstracts of all studies. Additional analyses of keywords retrieved from the Centre for Science and Technology Studies (CWTS, https://www.cwts.nl/) are reported in supplementary material SM1. All data included in the initial candidate set were subjected to analysis in VOSviewer (co-occurrence map with parameters set to binary counting, minimum number of occurrences set to 15, maximum number of keywords set to 200. Age-related and generic terms were excluded. The list of excluded keywords and map files to recreate the reported co-occurrence map can be found on OSF: https://osf.io/f7zdq/). Figure 3 displays the co-occurrence map between commonly mentioned keywords in our dataset (online interactive version of the figure: https://bit.ly/3yDPMup). 

![**Figure 3.** VOSviewer co-occurrence map of substantive keywords retrieved from the title and abstract of articles in our dataset. Colors represent VOSviewer-defined clusters of closely related keywords. See @VanEck2014 for further details on clustering in VOSviewer.](vosviewer_label_cooccurence.png)

The VOSviewer co-occurrence map corroborates that themes commonly studied in social neuroscience frequently co-occur in the titles and abstracts of articles in our data. Further, the analysis shows that individual topics could be organized into larger categories based on keyword co-occurrence clusters [represented as keyword colors in Figure 3; @VanEck2014]. As expected from a set of articles sampled from social neuroscience, these categories center around themes such as face perception (purple cluster), judgment and decision-making (green cluster), language (red cluster), and social pain/ostracism/exclusion (blue cluster). The default mode network (yellow cluster) also has clear ties to social neuroscience research [@Li2014].

Converging lines of evidence suggest that our search strategy and selection process was successful in curating a dataset both representative of, and exclusive to our target population of healthy human social fMRI research. Note that our sampling and selection process was largely constructed to overcome the problem that social fMRI is not a well-defined bibliometric category. Determining an initial set of candidates will likely be more straightforward when the field of interest aligns more closely with a well-defined bibliometric category (e.g., a WoS field category). 

We subsequently set out to quantitatively estimate the replication value for each study in this set (see Figure 1, step 2). Following @Isager2021 we chose *RV~Cn~* as our operationalization of replication value (equation 1). To quantify the replication value, researchers need to specify what function w should be used to weigh the citations, which type of citation impact $C$ is used, as well as source $S$ of that citation impact, if multiple sources are available. In the sections below we explain how citation impact and sample size data were collected in practice, and we explore the reliability of the collected data. 

### Operationalizing value as citation impact

To explore the impact of choosing one specification over another, we studied the reliability of citation impact estimates across a range of impact types $C$, sources $S$, and functions $w$. Although changes to these values will immediately impact the absolute replication value that is calculated, we are mainly interested in their impact on the relative ranking of studies in terms of replication value. Two qualitatively different types of citation impact C were collected; traditional academic citation indexes and Altmetric attention scores. Altmetric attention scores were collected using the *rAltmetric* package in R [@Ram2017; download date: 2020-10-30]. Altmetric attention scores are a weighted count of news- and social-media attention an article has received. For traditional citation impact, we collected data from multiple sources, including WoS (collected 2020-11-07 using the WoS web interface), Crossref [collected 2020-10-30 using the rCrossref package in R; @Chamberlain2020], Scopus [collected 2020-10-30 using the rScopus package in R; @Muschelli2019], CWTS (collected 2020-10-28 from the CWTS database by author TvL), and scite™ (www.scite.ai; obtained 2021-08-23 by scite™ staff on request). WoS, Crossref, Scopus and scite™ citation counts are all unweighted raw counts of incoming citations of an article. CWTS citation counts consist only of incoming citations that are not self-citations. We also collected field- and age-normalized citation counts from the CWTS database. This normalization process corrects for differences between subfields in how often papers are cited on average, with the aim to treat publications from different fields equally [for details about the normalization procedure, and a discussion of the use of arithmetic averages in skewed distributions, see @Waltman2011]. The score represents how many more times the article is cited relative to the average citation count of an article in its field from the same year. Thus, our data contained three different functions $w$ of traditional citation impact (raw count, self-citations subtracted, and field/age-normalized). Publication year data $Y$ was collected from the WoS database. 

### Operationalizing uncertainty as sample size

Following the rationale of @Isager2021 we operationalized the uncertainty about a claim before replication in terms of the standard error of effects supporting the claim. The standard error can be computed based on the standard deviation and the sample size, which is a combination of the number of participants and the number of observations per participant. We originally aimed to collect multiple sources of information that are relevant to quantifying the uncertainty such as information about the statistical test and the test results (e.g., the standard deviation of the dependent variable), the experimental design (e.g., the number of trials), the number of existing replications, etc, as such information can be used to compute and evaluate alternative operationalizations of replication value. This information would allow us to compare estimates from the *RV~Cn~* indicator with other proposed indicators of replication value [e.g., @Field2019, which requires information about bayes factors]. We performed two pilot studies to 1) identify additional information that could be coded to quantify uncertainty, and 2) examine if this information could be efficiently coded (see supplementary materials SM2 and SM3, respectively). From these pilot studies we concluded that the additional information required identifying the main claim in a paper (which was often feasible based on the abstract), and, more importantly, the results from the statistical test that provided empirical support for the claim. The later proved difficult, as it was often not possible to identify which of many statistical tests authors reported was the basis of the main claim, and therefore we were unable to extract the relevant statistical information to compute the replication value. Furthermore, statistical results were often not reported in sufficient detail to extract information (e.g., about the standard deviation of the measure). We concluded that it would not be feasible to collect additional information related to the uncertainty of the claim on a large scale from the social fMRI literature. 

In the end, the number of participants was the only operationalization of uncertainty we were able to move forward with in this study. This is an approximation of uncertainty that ignored variation in standard deviations, and the number of trials in a study, which is a topic we will return to in the general discussion. The total number of participants in a study for which fMRI data was reported (i.e., the number of participants that were not excluded from all fMRI analyses) was manually coded by undergraduate students at Leiden University and Eindhoven University of Technology.

### Collecting and inspecting the reliability of *RV~Cn~* input

#### Reliability of citation impact across sources

To better understand the relationship between different variables related to the citation impact $C$ across sources $S$, we explored the strength of the association between a variety of citation metrics (Table 3).

All metrics were retrieved within a time span of two weeks to prevent differences due to a time-lag. Figure 4 displays the distributions of all citation metrics. All metrics are heavily right skewed. The distributions of raw citation counts are highly overlapping across sources (Figure 4A). CWTS citation counts are more heavily skewed towards zero than raw counts from other metrics, likely due to the fact that CWTS subtracts self-citations from the total citation count.

\

```{r tab3}
pander(t.cit.metrics, caption = "\\textbf{Table 3:} Frequency of various citation metrics available for our data. Web of Science citation counts were originally available for all articles, but some could not be retrieved when the citation count data was updated in 2020.")
```

\ 

```{r fig4, fig.cap="**Figure 4.** Density distribution of citation metrics up to 200 citations. **A)** The distribution of raw citation counts from Web of Science (black), Crossref (red), Scopus (blue) and CWTS (orange). **B)** The distribution of CWTS citation impact up to a score of 10, normalized by research field/cluster. **C)** The distribution of Altmetric attention scores up to 100.", warning=FALSE}

grid.arrange(g.raw, g.tncs, g.alt)
```

To examine how strongly WoS, Crossref, Scopus, CWTS, and scite™ were correlated measures of the same underlying construct - the raw academic citation impact of an article - we subjected the citation data from these sources to an intraclass correlation analysis [model = two-way fixed effects, type = single rater, definition = consistency; @Koo2016] using the ICC function in the R package *psych* [@Revelle2021; ICC3 output reported]. Because all citation metrics have a skewed distribution, and because we are primarily concerned with the rank-ordering of studies we retrieved citation metrics for [@Isager2020] Spearman’s rho correlation was used to assess the strength of association. 

Figure 5 displays the rank-order correlations between various citation metrics. The correlation between raw citation counts from any two sources was very high (always >`r min(cor.mat.citations[1:4, 1:4])`). The inter-rater reliability between these metrics was similarly high, ICC = `r icc.cit$results[3, "ICC"]`, CI95%[`r icc.cit$results[3, "lower bound"]`, `r icc.cit$results[3, "upper bound"]`]. When self-citations are subtracted, as is done in the CWTS citation counts, correlations are only ever so slightly slightly lower compared to intercorrelations between the other sources, suggesting that self-citations will not have a large impact the computation of a replication value. 


```{r fig5, fig.cap="**Figure 5.** Matrix of bi-variate correlations between the citation metrics available for the articles in our dataset. ", warning=FALSE}

corrplot(corr = cor.mat.citations,
         method = "color",
         col = col(200),
         addCoef.col = "black",
         tl.col="black",
         tl.srt=45,
         diag = F)
```

As expected based on the prior literature [@Costas2015] the correlations between Altmetric scores and all other metrics were consistently low. The correlation between normalized and non-normalized citation counts was consistently high across sources, though substantially lower than the inter-correlation between different raw citation counts. As will be discussed in more detail below, this suggests that it matters little for *RV~Cn~* estimates which source $S$ is used, but it does matter whether one chooses raw or field-normalized citation count as the operationalization of $w(C)$, and it would matter substantially whether one chooses to use traditional citation count or news/social-media impact as the operationalization of $C$. The reliability of Altmetric attention scores as estimates of news/social-media impact remains unclear, as we had no other metrics for this kind of impact to compare against. We will examine the consequences of using Altmetric scores or field-normalized citation counts on the computation of replication value scores below. 

#### Accuracy and unbiasedness of average yearly citation count.

The ideal citation metric of *RV~Cn~* is the number of *future* citations an article will receive [@Isager2021]. Total citation count would not be a useful estimator of future citation impact because citations accumulate over time. As an article gets older it will tend to get a higher total citation count regardless of expected future citations, which means that a 50 year old article cited once per year would have the total citations as an article published last year that has been cited 50 times, even though we should expect the latter to have much more impact on the field in the future. To prevent age from impacting the replication value of articles, *RV~Cn~* uses the average yearly citation count instead of the total count as an operational measure of value. 

To examine how well average yearly citation count predicts future citation count we obtained the yearly citation rate for each article in our dataset from scite™. Then, using the average yearly citation count of each article from using all years until 2019, we predicted the citation rate of each article in our data for year 2020 (the last complete year in the data). To examine whether average yearly citation count is a sufficient approach to correct for the effect of age on citation counts we examined the correlation between age and average yearly citation count. In addition, we explored the relationship between age-averaged citation count and age/field-normalized CWTS citation count, which are age-adjusted using the superior method of normalizing the citation count against all articles from the same year. If age-averaging is an effective method for age adjustment, age-averaged citation count should correlate more strongly with CWTS normalized scores than raw citation count. Finally, we also examined the effect of age-averaging on Altmetric attention scores. Our goal in examining the relationship between these variables is to gain a better understanding of which data should be used to quantify the value of a published study.

We focus on scite™ citation count data in these analyses since it was the only source from which we could obtain data on yearly citation rate. However, the reported pattern of results is highly similar regardless of which citation source is used (see supplementary material SM4). 

##### Predictive accuracy

Figure 6A displays the scite™ citation rate trajectory for all articles in our data. Figure 6B displays the same trajectories on a log+1 scale with box plots summarizing the distribution for each year since publication, which gives a better sense of the overall trend. On average, most articles seem to be cited at an increasing rate for about the first two years since publication. Then the citation rate stabilizes, possibly increasing slightly around year ten. Given this general trend, our auxiliary assumption that average yearly citation count is on average a useful predictor of future citation impact is supported. Including citations from the two first years seems to leads to an underestimation of the citation rate in later years, but this might not directly affect any rank-order of citation counts. 

```{r fig6, fig.cap="**Figure 6.** **A)** Citation trajectories for all articles in the dataset. **B)** Log citation trajectories, with box plot summaries for each year. **C)** Citations obtained in 2020 predicted by the average yearly citation count from the articles publication year until 2019. ", warning=FALSE}

lay <- rbind(c(1,2),
             c(3,3))
grid.arrange(p.traj_raw, p.traj_log, p.c2020_by_cy1, layout_matrix = lay)
```

Figure 6C displays the accuracy of average yearly citation count (using data until 2019) to predict the “future” citation count in 2020. Predictive accuracy is quite good, but far from perfect, $\rho$ = `r c2020_by_cy1$estimate`, CI95%[`r c2020_by_cy1$conf.int[1]`, `r c2020_by_cy1$conf.int[2]`]. As noted above, average yearly citation count consistently underestimates how many citations are obtained in 2020. The two first years since publication are included in the average yearly citation count, which tends to drag down the average. Also as expected, underestimation of citations in 2020 seem to be particularly severe for more recently published articles. The younger the article, the more its average yearly citation count is influenced by the relatively lower number of yearly citations in the two first years since publication. Because total citation counts obtained from scite™ were highly correlated with total citation count obtained from other sources, we believe the results reported here likely generalizes to citations from WoS, Crossref, Scopus, and CWTS. The results suggest that the predictive accuracy of *RV~Cn~* could be improved by excluding citations from the first two years since publication. However, such information is typically not available in most databases (it was provided to us by scite™ (www.scite.ai)). Alternatively, accuracy could be improved through more accurate modeling of each article’s citation trend. However, such improvements require data on citations per year, which is not easily accessible to most researchers.

##### Predictive unbiasedness

Article age was very weakly correlated with the number of scite™ citations an article received from january to december of the year 2020, $\rho$ = `r c2020_by_age$estimate`, CI95%[`r c2020_by_age$conf.int[1]`, `r c2020_by_age$conf.int[2]`], suggesting article age is not a determinant of future citation impact and can safely be corrected for. To examine how well age-averaging corrects citation estimates for age, we computed pairwise spearman correlations between publication age, scite™ citation count, Altmetric scores, scite™ citation count divided by years since publication, Altmetric scores divided by years since publication, and CWTS normalized citation count.

```{r fig7, fig.cap="**Figure 7.** Matrix of bi-variate correlations between age and citation indices.", warning=FALSE, echo=FALSE}

corrplot(corr = cor.mat.age,
         method = "color",
         col = col(200),
         addCoef.col = "black",
         tl.col="black",
         tl.srt=45,
         diag = F)
```

Figure 7 displays the correlation coefficients between all variables of interest. Not surprisingly, there was a strong correlation between age and raw scite™ citation count, $\rho$ = `r cit_by_age$estimate`, CI95%[`r cit_by_age$conf.int[1]`, `r cit_by_age$conf.int[2]`]. The correlation between citations and age dropped substantially when citation count was divided by years since publication. However, a small residual correlation between average yearly citation rate and publication age remains, $\rho$ = `r ycit_by_age$estimate`, CI95%[`r ycit_by_age$conf.int[1]`, `r ycit_by_age$conf.int[2]`]. This suggests that dividing total citation count by the number of years since publication is an imperfect age adjustment method, but the correction substantially reduces the correlation between age and citation count, and is therefore a substantially better measure than total citation counts. Averaging over age works best if citation time accumulates at a constant rate, but this rate quite variable for most articles (Figure 6A). Encouragingly, however, averaging citation count by age does increase the correlation between citation count and CWTS normalized scores, whose method of age correction is superior as it corrects for the the average number of citations of all publications published in the same field and in the same year. Interestingly, even CWTS scores are weakly positively correlated with age, suggesting that perfectly adjusting for article age is challenging. In summary, taking the average yearly citation count seems to be an imperfect but efficient method for age adjustment in traditional citation metrics.

#### Coding number of participants

The number of participants for each study in our dataset was coded manually. Manually coding the number of participants for all studies in the full set of `r nrow(data.bib)` candidate articles was assumed to be costly and time consuming from the outset. In practice, we expect most researchers to have more narrow inclusion criteria when computing the replication value for a set of replication targets. For feasibility reasons, we aimed at coding 1000 articles at random from the full set of `r nrow(data.bib)` articles and began the process of splitting these into individual studies for coding the number of participants. While coding, it became clear that many studies did not meet our inclusion criteria, and additional sample of 500 articles were sampled at random from the full set. The exact code used to draw the sample is available on OSF (https://osf.io/rxukq/). After removing articles that matched our initial exclusion criteria [e.g., single non-fMRI studies from multi-study articles, such as @DeVries2018, study 4] the number of participants was coded for each fMRI study in the article.

Coding was performed by a team of three undergraduate research assistants. For each article we identified the number of studies reported in the article. For each study we recorded the number of participants who contributed any fMRI data to analyses reported in the study (even if their data were excluded from some analyses). We did not code. Piloting efforts suggested it would be too difficult to extract more detailed sample size information such as the number of stimuli and trials used in each study information for over 1000 studies, even though such information would enable a more accurate quantification of statistical uncertainty [@Westfall2014]. For further details about how coders were instructed to proceed with coding the number of participants, see the supplementary coding instructions (https://osf.io/j3pxf/).

The 1500 articles contained `r nrow(data.all.raw)` individual studies, of which `r sum(data.all.raw$excluded, na.rm = T)` matched our exclusion criteria. The final dataset contained `r nrow(data.all)` individual studies from `r length(unique(data.all$DI))` unique articles. Coding time was a few minutes when the number of participants and exclusion criteria were clearly summarized in either the study abstract or the “participants” subsection of the methods section, but could take longer if reporting was less structured. In order to ensure that the number of participants was reliably coded, a subset of `r nrow(data.irr)` studies, randomly selected from the larger set of `r nrow(data.all)`, were double-coded by independent coders and subjected to an inter-rater reliability analysis. Two additional coders (one additional undergraduate student - the undergraduate coder - and the first author - the PhD coder) re-coded the number of participants for each study in this subset. While coding, all coders were blind to the number of participants provided by other coders. To study inter-rater reliability, we subsequently calculated the percentage agreement between each of the coders, and we calculated the intraclass correlation coefficient between coders (model = one-way fixed effects, type = single rater, definition = absolute agreement) using ICC function in the R package *psych* (ICC3 output reported). Overall, there was a high but imperfect agreement between the three coders (percentage exact agreement = `r all.match`). The intraclass correlation coefficient between raters was high, ICC = `r icc.n$results[3, "ICC"]`, CI95%[`r icc.n$results[3, "lower bound"]`, `r icc.n$results[3, "upper bound"]`]. Figure 8 displays the variation in sample size between the coders, plotted on log scale.

```{r fig8, fig.cap="**Figure 8.** Variation in sample size between coders. Sample size is plotted on log scale. The original sample size coded is represented on the x-axis. Double-coded sample size values are represented on the y-axis. Blue circles represent values from the PhD-student coder. Brown triangles represent values from the undergraduate student coder.", warning=FALSE}

plot(g.irr)
```

Coders disagreed in `r sum(data.irr$matches_BA_PhD==0 | data.irr$matches_orig_PhD==0 | data.irr$matches_BA_PhD==0)` cases. All disagreements between coders were resolved by the PhD coder after inspecting comments by the other coders. In addition to the cases of disagreements identified in the data used for inter-rater reliability analysis, one additional sample size coding error in the full set of `r nrow(data.all)` studies was detected and corrected at a later time during the analyses. Figure 9 displays the distribution of sample size in our data after resolving coder disagreements (mode=`r mode.samplesize`, median=`r median.samplesize`, frequency of *n<=10*=`r zerototen`, *11-20*=`r tentotwenty`, *21-30*=`r twentytothirty`, *31-40*=`r thirtytofourty`, *41-50*=`r fourtytofifty`, *51-60*=`r fiftytosixty`, *61-70*=`r sixtytoseventy`, *71-80*=`r seventytoeighty`, *81-90*=`r eightytoninety`, *91-100*=`r ninetytohundred`, *n>100*=`r overhundred`).

```{r fig9, fig.cap="**Figure 9.** Distribution of sample sizes in the dataset. For visualization purposes, the x-axis limit is set to n = 200.", warning=FALSE}

plot(g.samplesize)
```

### Calculating and comparing alternative operationalizations of *RV~Cn~*

Having established that sufficiently accurate citation counts and participant numbers can be collected, we proceeded with the calculation of *RV~Cn~*. Because traditional citation counts are not strongly associated with Altmetric attention scores, replicating researchers might justifiably want to use either or both of these metrics to estimate value. Therefore, we decided to compare the results of two alternative operationalizations of replication value; one indicator measured value via the Web of Science citation count of the articles (*RV~WoS~*), while the other indicator measured value via Altmetric score of the articles (*RV~Alt~*). Both indicators used sample size as a measure of uncertainty. Note that we also explore the relationship between *RV~WoS~* and other data sources, such as field-normalized citation impact and citation counts from scite™ and Scopus. 

*RV~WoS~* was based on the equations derived by @Isager2021, and calculated in the following way:

\begin{equation} 
  \tag{2}
  RV_{WoS} = \frac{C_{WoS}}{Y+1}\times\frac{1}{\sqrt{n}}
  (\#eq:2)
\end{equation}

where $C_{WoS}$ denotes the Web of Science citation count of the article a study is reported in, $Y$ denotes the article age in years, and $n$ denotes the sample size of the study after exclusion.

*RV~Alt~* was calculated in the following way:

\begin{equation} 
  \tag{3}
  RV_{WoS} = C_{Alt}\times\frac{1}{\sqrt{n}}
  (\#eq:3)
\end{equation}

where $C_{Alt}$ denotes the Altmetric attention score of the article, and $n$ denotes the sample size of the study after exclusion. Because the analyses above revealed that Altmetric attention scores are not strongly correlated with article age in our data, we did not average $C_{Alt}$ over publication year in this replication value indicator. MAny articles are not mentioned in any sources that are tracked by Altmetric, and therefore have a score of 0. In our dataset $C_{Alt}$ could only be calculated for `r sum(!is.na(data.all$RV_alt))` of `r nrow(data.all)` studies.

Importantly, we calculated both *RV~WoS~* and *RV~Alt~* under the assumption that no study in our candidate set is a replication of another study in the set, implying that no studies should be combined in the estimate of n. Because lack of replication research in fMRI research [@Poldrack2017] implies that only very few articles in our dataset would be replications of one another, we found it acceptable to proceed with calculation under the assumption that there were no replications in the data. Where direct replication studies have been performed, it would have been more appropriate to combine the sample size from the original study and it’s replications [@Isager2021, supplementary material 1]. However, there are no databases that store information about direct replication in social neuroscience. Whenever researchers compute the replication value for a more specific population, information about direct replications might be more readily available, or it can be manually coded. 

The distribution of replication value from both indicators was visually inspected, and estimates from both indicators were correlated to study their similarity. Spearman’s rho was used since the rank-order correlation between different indicators is of primary interest. 95% bootstrap confidence intervals were calculated for the correlation estimate using the spearman.ci function of the RVAideMemoire package in R [@Herve2021].

```{r fig10, fig.cap="**Figure 10.** Scatter plot visualizing the relationship between *RV~WoS~* and *RV~Alt~*. Distribution of *RV~WoS~* estimates are visualized as bars on the x-axis. Distribution of *RV~Alt~* estimates are visualized as bars on the y-axis. Blue bars (and dots) represent the 10 highest *RV~Alt~* scores. Red bars (and dots) represent the 10 highest *RV~WoS~* scores. Purple dots represent scores that are among the 10 highest scores on both estimators. Two of the ten studies with the highest *RV~WoS~* scores are not included in the scatter plot because their *RV~Alt~* scores could not be computed due to missing Altmetric attention scores.", warning=FALSE}
lay <- rbind(c(1,2),
             c(3,4))
grid.arrange(RVcor.p, RVcor.2, RVcor.3, RVcor.4, layout_matrix = lay)

```

Figure 10 displays the distribution of *RV~WoS~*, *RV~Alt~*, *RV~Scopus~*, *RV~tncs~* (field-normalized citation scores), *RV~scite~*, and their associations with *RV~WoS~*. Overall, all distributions are highly skewed with most scores distributed around low values, which is expected given that the number of participants, citation counts, and Altmetric attention scores are all highly skewed as well (see Figure 4 and Figure 9). Overall rank-order correlations were high for different citation sources (Wob of Science, Scopus, scite), lower for field-normalized citation counts, and low for Altmetric scores (see Figure 11). As a consequence, only two studies [@Tamir2012; @Kassam2013] were ranked among the top ten in both Wob of Science and Altmetric rank-orderings (purple-colored points in Figure 10). The same was true for field-normalized citation scores, where the overlap between top-ranked studies using Web of Science citation scores and field-normalized citation scores was very low (despite the relatively high correlations between the two measures). Traditional citation impact and altmetric attention scores are generally thought to measure different aspects of impact and are known to be weakly associated. It is clear field-normalized citation scores also measure impact in a substantially different manner than raw citation counts. The overlap between citation counts from different sources such as scite™ or Scopus does not lead to substantially different selections, even though even there some variation in the last one or two studies included when selecting the X highest ranked studies (e.g., the 9th and 10th study included in a Top 10) should be expected to vary.

To conclude, quantitative recommendations for which studies to replicate will vary substantially based on whether traditional, field-normalized, or altmetric citation impact is used to estimate replication value, because these impact metrics measure non-overlapping aspects of scientific impact. Different stakeholders may prefer either operationalization, depending on what aspects of impact they find most relevant. Altmetric attention scores are only weakly correlated with traditional citation counts, which has a substantial impact on *RV~Cn~* estimates. 

```{r fig11, fig.cap="**Figure 11.** Matrix of bi-variate correlations between replication value indices computed based on different operationalizations of value through citations or Altmetrics.", warning=FALSE, echo=FALSE}

corrplot(corr = cor.mat.rv,
         method = "color",
         col = col(200),
         addCoef.col = "black",
         tl.col="black",
         tl.srt=45,
         diag = F)
```

## Step 3 - In depth review of recommended candidates

The final step when selecting a replication target is an in-depth inspection of studies with a high replication value. We expect such an in-depth review to reveal certain boundary conditions of when the number of participants and/or the citation count do not accurately reflect the value and impact of a study. We subjected the 10 studies with the highest and lowest replication value on either *RV~WoS~* or*RV~Alt~* to an in-depth inspection. In addition, we included the 10 lowest non-zero estimates from the *RV~WoS~* distribution, because *RV~WoS~* scores of 0 often simply reflect a paper too young to have picked up citations yet. In total, 44 unique studies were included in our face validity review (6 studies were among the highest or lowest scores for both indicators).We wanted to see whether quantitative replication value estimates would conform to our own intuitions about replication value, and identify factors that would lead to a high replication value using a formula-based approach, without actually warranting a replication. Such boundary conditions are likely present in other sets of replication targets as well, and identifying such factors will help researchers during the in-depth inspection in step 3. For example, an article may be highly cited for reasons other than the empirical studies it reportes, which would lead to a highly cited paper while the study in the article is not worth replicating. As such, the goal is to identify potential issues with validity, reliability and measurement error that future validation studies of *RV~Cn~* may want to follow up on.

Authors PMI and AvtV read the title and abstracts of all studies included in the review, consulted the article text intermittently for clarifications, and reviewed quantitative information related to the replication value estimates of these studies (i.e., reviewers were not blinded to a record’s rank position). Both reviewers first made notes for each study in private, focusing on their intuitive validity judgment of the replication value estimate and on potential sources of error and bias. Notes were then discussed by PMI, AvtV, and DL in two meetings to distill the most central outcomes of the review effort. The full set of notes is available on OSF for author PMI (https://osf.io/vwpqs/) and AvtV (https://osf.io/953rh/).

### Central outcomes of the review process

The in-depth review yielded several insights. A detailed inspection of quantitative replication value estimates is important for quality control. In two studies, coders had erroneously coded an incorrect number of participants (due to a transcription error, and overlooking data exclusions). Eight articles turned uout not to be connected to social neuroscience, and one study did not utilize fMRI for imaging. Finally, in one case we had incorrectly labeled a single two-session repeated measures study as two separate studies. 

There was not always an intuitive correspondence between the *RV~WoS~*/*RV~Alt~* rank order and our intuitions about the replication value of the claims purely based on the title and information in the abstract. One lack for the correspondence was because reviewers were not blind to the replication value ranking, and had access to the citation count and number of participants, which were so salient they were difficult to not take into account. Another reason was that without other explicit criteria determining to determine the value of a replication study, there was substantial subjectivity in the value of each study as judged by both reviewers. This is not unexpected, as peer evaluations of the value of a study is variable, and not strongly related to eventual citation scores [@gottfredson_evaluating_1978]. A final reason for the low perceived correspondence between indicator estimates and reviewer intuitions were a number of boundary conditions where the *RV~WoS~*/*RV~Alt~* estimates did not accurately reflect the value and uncertainty of the studies. 

The first boundary condition was that many studies used within-subject designs, where the number of participants does not fully capture the uncertainty, as it ignores the number of measurements per participant. The use of within-subject designs seemed to be common among the highest ranked studies, as such designs require less participants for high statistical power, and therefore get a higher replication value when uncertainty is based only on the number of participants. This is clearly an important limitation, especially when the number of trials in each study varies substantially between studies (as was the case in the set of studies we examined). In future applications of *RV~Cn~*-based study selection we therefore recommend that uncertainty is quantified during step 2 based on both the number of participants, and the number of observations per participant. If this is unfeasible (which is likely given how unsystematically this information is reported in the literature), the number of observations should be taken into account during step 3 [see supplementary material 2 in @Isager2021 for technical details on how such a correction can be achieved]. Another boundary condition concerned a study that already had been replicated in the literature. Although rare, when replication studies already exist, the replication value should be computed based on the uncertainty remaining after all replication studies [@Isager2020].

Other boundary conditions concerned the reason an article is highly cited. One article containing both a literature review and an empirical study seemed to be cited primarily due to the literature review [@Dimoka2011]. Another study on human navigation appeared to receive a large Altmetric score primarily due to speculative news reports claiming that GPS use can “turn the brain off” - even though this conclusion did not follow from the study [@Javadi2017]. Researchers might want to reflect on whether the solution to a dubious interpretation of the results of a study warrant a replication study. The boundary conditions identified so far seem general enough to incorporate in the in-depth review process of replication targets by default. Future research should give us a better understanding of which additional factors to consider during in-depth review of replication candidates [e.g., @pittelkow_process_2023]. 

# General discussion

The overall aim of this exploratory study was to test the feasibility of implementing the four-step replication study selection procedure based on *RV~Cn~* proposed by @Isager2021 in social fMRI research. The current exploratory report reveals the importance of testing the feasibility of applying recently proposed selection strategies for replication studies, as well as carefully examining possible measures, auxiliary assumptions, and boundary conditions. We show it is possible to calculate *RV~Cn~* for a large candidate set of studies identified based on bibliometric information. We were able to reliably code the total number of participants and retrieve citation count data for each study in order to calculate *RV~Cn~* (step 2 in Figure 1). However, we were only able to code uncertainty coarsely as 'number of participants in study', omitting the number of trials per participant, which also determined the standard error of the estimate [@Westfall2014]. 

Traditional citation count metrics were highly rank-order correlated, meaning there is little difference in which source $S$ is used in the calculation of *RV~Cn~*. Field-normalized citation counts provide a slightly different measure of citation impact, and lead to less overlap in the final rank-order than non-normalized citation scores, especially in an interdisciplinary research topic such as social neuroscience, which is published in different scientific domains. Altmetric attention scores are weakly correlated with traditional citation impact, and represent a qualitatively different approach to measuring value. Whichever measure is preferred, both Altmetric scores, traditional citation counts could easily be extracted using free and open source applications [e.g., @Ram2017; @Chamberlain2020], where field-normalized citation counts or citation counts per year are not publicly available. 

Finally, in-depth review of the highest ranking indicator estimates from step 2 appears to be an important method of quality control before a candidate is selected for replication. This review revealed important boundary conditions of using citation counts and the total number of participants as measures of value and uncertainty. Auxiliary hypotheses, such that past citation counts predict future citation counts, that the source of the citation counts do not substantially affect the rank-order, and that we can control for the age of the article, were all supported. 

Whether these conclusions generalize to application of *RV~Cn~* in other disciplines is an open question which will need to be empirically examined. Our initial set of replication targets was large and heterogeneous, and the use of *RV~Cn~* might be more straightforward in more homogenous literatures, especially if these mainly rely on between-participant designs. Ideally, we would validate any quantification of replication value by benchmarking the quantitative estimates against a reliable measure of expected utility gain. However, such benchmark validation will be highly difficult to implement in practice, as utility judgments are difficult to quantify, and would require extensive discussions among peers in a specific research area to reach consensus about which studies are most valuable to replicate. 

It is possible to attempt to provide criterion validation of *RV~Cn~* by investigating whether *RV~Cn~* is associated with other operational measures that are hypothesized to predict expected utility gain. This would be possible if we assume that the measures *RV~Cn~* is compared with have a causal structure that makes them both valid measures of expected utility gain and correlated with *RV~Cn~* [note that one does not necessarily follow from the other per the definition of validity proposed in @Isager2020a]. For example, we would expect *RV~Cn~* to predict which studies are chosen for replication in practice under the assumption that both *RV~Cn~* and the selection criteria used by researchers who perform replication studies are caused by the expected utility of the replication effort [@Isager2021]. 

It might also be possible to validate *RV~Cn~* by examining the extent to which *RV~Cn~* predicts subjective estimates of the relative replication value of a set of studies. Future studies could also aim to  increase the understanding of which factors researchers usually consider when selecting a study for replication. Recently, @pittelkow_process_2023 identified a number of criteria suchas interest, doubt, impact, methodology, and feasibility. An important next step is to examine how feasible it is to quantify these factors, and how well such judgments relate to *RV~Cn~*. It is important that measures of *RV~Cn~* should not be predictive of whether a study will replicate successfully. *RV~Cn~* estimates should be the highest for studies we cannot accurately predict replication outcomes for, and therefore studies that are either very likely to replicate or very unlikely to replicate should *both* receive low *RV~Cn~*. 

The task of evaluating the uncertainty in scientific claims would have been made easier if researchers adhered to reporting standards, and when the relationship between statistical tests and scientific cliams were more clearly specified in the article [@appelbaum_journal_2018; @lakens_improving_2021]. Furthermore, it would be beneficial if researchers who perform replication studies are explicit about the reasons they selected a target study [@pittelkow_process_2023]. By exploring and documenting the wealth of information relevant to replication study selection, we can increase the ability of researchers to make well-informed decisions about which original research would be the most important to replicate.

# References



